{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EsLmOfflWRu",
        "outputId": "7f764db4-8abd-4519-fd40-8999bb4e7b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy\n",
            "  Downloading dspy-3.0.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting backoff>=2.2 (from dspy)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy) (1.5.2)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from dspy) (2.9.0)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy) (2025.11.3)\n",
            "Requirement already satisfied: orjson>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (3.11.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from dspy) (4.67.1)\n",
            "Collecting optuna>=3.4.0 (from dspy)\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting magicattr>=0.1.6 (from dspy)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting litellm>=1.64.0 (from dspy)\n",
            "  Downloading litellm-1.80.9-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting diskcache>=5.6.0 (from dspy)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting json-repair>=0.30.0 (from dspy)\n",
            "  Downloading json_repair-0.54.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy) (9.1.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy) (4.12.0)\n",
            "Collecting asyncer==0.0.8 (from dspy)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (6.2.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (3.1.2)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from dspy) (13.9.4)\n",
            "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (11.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (2.0.2)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (3.6.0)\n",
            "Collecting gepa==0.0.17 (from gepa[dspy]==0.0.17->dspy)\n",
            "  Downloading gepa-0.0.17-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (3.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (8.3.1)\n",
            "Collecting fastuuid>=0.13.0 (from litellm>=1.64.0->dspy)\n",
            "  Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting grpcio<1.68.0,>=1.62.3 (from litellm>=1.64.0->dspy)\n",
            "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (4.25.1)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (1.2.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (0.22.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy) (1.3.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (1.17.2)\n",
            "Collecting colorlog (from optuna>=3.4.0->dspy)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (2.0.44)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (6.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (1.22.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy) (1.3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm>=1.64.0->dspy) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.64.0->dspy) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->dspy) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.64.0->dspy) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy) (0.30.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy) (3.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm>=1.64.0->dspy) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy) (1.2.0)\n",
            "Downloading dspy-3.0.4-py3-none-any.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.2/285.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading gepa-0.0.17-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.54.2-py3-none-any.whl (29 kB)\n",
            "Downloading litellm-1.80.9-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: magicattr, json-repair, grpcio, gepa, fastuuid, diskcache, colorlog, backoff, asyncer, optuna, litellm, dspy\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.76.0\n",
            "    Uninstalling grpcio-1.76.0:\n",
            "      Successfully uninstalled grpcio-1.76.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asyncer-0.0.8 backoff-2.2.1 colorlog-6.10.1 diskcache-5.6.3 dspy-3.0.4 fastuuid-0.14.0 gepa-0.0.17 grpcio-1.67.1 json-repair-0.54.2 litellm-1.80.9 magicattr-0.1.6 optuna-4.6.0\n",
            "✅ DSPy configuration complete. Using dspy.LM with LongCat-Flash-Chat.\n"
          ]
        }
      ],
      "source": [
        "!pip install dspy pydantic requests beautifulsoup4 pandas\n",
        "\n",
        "import dspy\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "LONGCAT_API_KEY = \"ak_1jN8Ei49i9Po5OR7Qs0Ta51Y3ls4s\"\n",
        "\n",
        "\n",
        "try:\n",
        "    lm = dspy.LM(\n",
        "        model='openai/LongCat-Flash-Chat',\n",
        "        api_key=LONGCAT_API_KEY,\n",
        "        api_base=\"https://api.longcat.chat/openai/v1\",\n",
        "        max_tokens=4096\n",
        "    )\n",
        "    dspy.settings.configure(lm=lm)\n",
        "    print(\"✅ DSPy configuration complete. Using dspy.LM with LongCat-Flash-Chat.\")\n",
        "except Exception as e:\n",
        "\n",
        "    print(f\"❌ CRITICAL ERROR: LLM setup failed. Please confirm your key is correct. Full Error: {e}\")\n",
        "\n",
        "\n",
        "ALL_URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
        "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import dspy\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "class EntityWithAttr(BaseModel):\n",
        "    entity: str = Field(description=\"the named entity (e.g., 'sustainable agriculture')\")\n",
        "    attr_type: str = Field(description=\"semantic type (e.g. Concept, Process, Technology, Drug, Disease)\")\n",
        "\n",
        "class Triple(BaseModel):\n",
        "    subject: str = Field(description=\"The subject of the relationship (must be a canonical entity from the provided list).\")\n",
        "    predicate: str = Field(description=\"The relationship/verb connecting the subject and object (e.g., 'causes', 'uses', 'affects').\")\n",
        "    object: str = Field(description=\"The object of the relationship (must be a canonical entity from the provided list).\")\n",
        "\n",
        "class DeduplicationResult(BaseModel):\n",
        "    deduplicated_list: List[EntityWithAttr] = Field(description=\"The final list of unique, canonical entities.\")\n",
        "    confidence: float = Field(description=\"The LLM's confidence score (0.0 to 1.0) that the list is perfectly deduplicated. Must be 0.9 or higher.\")\n",
        "\n",
        "\n",
        "class ExtractEntities(dspy.Signature):\n",
        "    \"\"\"Extract all relevant named entities and their semantic types from the provided paragraph.\"\"\"\n",
        "    paragraph: str = dspy.InputField()\n",
        "    entities: List[EntityWithAttr] = dspy.OutputField()\n",
        "\n",
        "class DeduplicateTags(dspy.Signature):\n",
        "    \"\"\"Given a list of noisy entities, deduplicate them into a canonical list and assess confidence.\"\"\"\n",
        "    items: List[EntityWithAttr] = dspy.InputField(desc=\"Initial list of entities, often containing duplicates and varying spellings.\")\n",
        "    deduplicated_result: DeduplicationResult = dspy.OutputField()\n",
        "\n",
        "class ExtractTriples(dspy.Signature):\n",
        "    \"\"\"Extract all semantic triples (subject, predicate, object) from the text using only the provided canonical entities as subjects and objects.\"\"\"\n",
        "    paragraph: str = dspy.InputField()\n",
        "    canonical_entities: str = dspy.InputField(desc=\"A comma-separated list of canonical entity names. Use ONLY these entities as subjects and objects in the triples.\")\n",
        "    triples: List[Triple] = dspy.OutputField()\n",
        "\n",
        "class DeduplicatorWithConfidence(dspy.Module):\n",
        "    def __init__(self, target_confidence=0.9):\n",
        "        super().__init__()\n",
        "        self.target_confidence = target_confidence\n",
        "        self.dedup_predictor = dspy.Predict(DeduplicateTags)\n",
        "        self.max_attempts = 5\n",
        "\n",
        "    def forward(self, items: List[EntityWithAttr]) -> List[EntityWithAttr]:\n",
        "\n",
        "        attempt = 0\n",
        "        while attempt < self.max_attempts:\n",
        "            pred = self.dedup_predictor(items=items)\n",
        "            result = pred.deduplicated_result\n",
        "\n",
        "            if result.confidence >= self.target_confidence:\n",
        "                return result.deduplicated_list\n",
        "\n",
        "            print(f\"    [DEDUPLICATOR] Low confidence ({result.confidence:.2f} < {self.target_confidence}). Retrying...\")\n",
        "            attempt += 1\n",
        "\n",
        "        print(f\"    [DEDUPLICATOR] Failed to reach {self.target_confidence} confidence after {self.max_attempts} attempts. Returning last result (Confidence: {result.confidence:.2f}).\")\n",
        "        return result.deduplicated_list\n",
        "\n",
        "def fetch_url_content(url):\n",
        "    \"\"\"Fetches text content from a URL using BeautifulSoup.\"\"\"\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, timeout=15, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        for element in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
        "            element.decompose()\n",
        "\n",
        "        text = soup.get_text()\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return cleaned_text[:10000]\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        print(f\"Error fetching {url}: {error_message}\")\n",
        "        return None\n",
        "\n",
        "def triples_to_mermaid(triples: List[Triple], entity_list: List[str]) -> str:\n",
        "    \"\"\"Converts a list of Triple objects into a valid Mermaid graph definition.\"\"\"\n",
        "    entity_set = {e.strip().lower() for e in entity_list}\n",
        "\n",
        "    def _clean_node(name):\n",
        "        return re.sub(r'[^a-zA-Z0-9_-]', '', name.replace(' ', '_').lower())\n",
        "\n",
        "    def _clean_label(label):\n",
        "        trimmed_label = label[:40].replace('\"', \"'\")\n",
        "        return trimmed_label\n",
        "\n",
        "    lines = [\"graph TD\"]\n",
        "\n",
        "    node_id_map = {}\n",
        "    node_counter = 0\n",
        "    for entity in sorted(list(entity_set)):\n",
        "        clean_id = f\"N{node_counter}_{_clean_node(entity)}\"\n",
        "        node_id_map[entity] = clean_id\n",
        "        lines.append(f\"    {clean_id}[\\\"{entity.title()}\\\"]\")\n",
        "        node_counter += 1\n",
        "\n",
        "    for triple in triples:\n",
        "        src = triple.subject.strip().lower()\n",
        "        dst = triple.object.strip().lower()\n",
        "        lbl = triple.predicate.strip()\n",
        "\n",
        "        if src in entity_set and dst in entity_set:\n",
        "            src_id = node_id_map[src]\n",
        "            dst_id = node_id_map[dst]\n",
        "            lines.append(f\"    {src_id} -- \\\"{_clean_label(lbl)}\\\" --> {dst_id}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def process_url(url_index: int, url: str, extractor, dedup_mod, triple_extractor) -> tuple[list, str]:\n",
        "    \"\"\"\n",
        "    Runs the full pipeline for a single URL.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Processing URL {url_index}: {url} ---\")\n",
        "\n",
        "    content = fetch_url_content(url)\n",
        "\n",
        "    if not content:\n",
        "\n",
        "        print(f\"Skipping URL {url_index} data processing due to fetch error.\")\n",
        "        return [], \"\"\n",
        "\n",
        "    print(\"  -> Step 1: Extracting noisy entities...\")\n",
        "    try:\n",
        "        extracted_entities_pred = extractor(paragraph=content)\n",
        "        noisy_entities: List[EntityWithAttr] = extracted_entities_pred.entities\n",
        "        print(f\"    Extracted {len(noisy_entities)} initial entities.\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Error during entity extraction: {e}\")\n",
        "        return [], \"\"\n",
        "\n",
        "    print(\"  -> Step 2: Deduplicating entities with confidence check...\")\n",
        "    canonical_entities = dedup_mod(items=noisy_entities)\n",
        "    print(f\"    Deduplicated to {len(canonical_entities)} canonical entities.\")\n",
        "\n",
        "    canonical_entity_names = [e.entity.strip() for e in canonical_entities]\n",
        "    canonical_entity_str = \", \".join(canonical_entity_names)\n",
        "\n",
        "    print(\"  -> Step 3: Extracting semantic triples...\")\n",
        "    try:\n",
        "        triples_pred = triple_extractor(\n",
        "            paragraph=content,\n",
        "            canonical_entities=canonical_entity_str\n",
        "        )\n",
        "        triples: List[Triple] = triples_pred.triples\n",
        "        print(f\"    Extracted {len(triples)} valid triples.\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Error during triple extraction: {e}\")\n",
        "        triples = []\n",
        "\n",
        "    print(\"  -> Step 4: Generating Mermaid diagram...\")\n",
        "    mermaid_code = triples_to_mermaid(triples, canonical_entity_names)\n",
        "\n",
        "    filename = f\"mermaid_{url_index}.md\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(mermaid_code)\n",
        "    print(f\"  -> File saved: {filename}\")\n",
        "\n",
        "    return canonical_entities, mermaid_code\n",
        "\n",
        "ALL_URLS = [\n",
        "    'https://en.wikipedia.org/wiki/Sustainable_agriculture',\n",
        "    'https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare',\n",
        "    'https://en.wikipedia.org/wiki/Climate_change',\n",
        "    'https://medlineplus.gov/druginformation.html',\n",
        "    'https://www.fao.org/3/y4671e/y4671e06.htm',\n",
        "    'https://en.wikipedia.org/wiki/Astrophysics',\n",
        "    'https://en.wikipedia.org/wiki/Pharmacovigilance',\n",
        "    'https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets',\n",
        "    'https://www.nps.org.au/consumers/medicine-and-side-effects',\n",
        "    'https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india'\n",
        "]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    entity_extractor = dspy.Predict(ExtractEntities)\n",
        "    deduplicator = DeduplicatorWithConfidence(target_confidence=0.9)\n",
        "    triple_extractor = dspy.Predict(ExtractTriples)\n",
        "\n",
        "    all_csv_rows = []\n",
        "\n",
        "    for i, url in enumerate(ALL_URLS, 1):\n",
        "\n",
        "        canonical_entities, mermaid_output = process_url(\n",
        "            url_index=i,\n",
        "            url=url,\n",
        "            extractor=entity_extractor,\n",
        "            dedup_mod=deduplicator,\n",
        "            triple_extractor=triple_extractor\n",
        "        )\n",
        "\n",
        "\n",
        "        for entity in canonical_entities:\n",
        "            all_csv_rows.append({\n",
        "                'link': url,\n",
        "                'tag': entity.entity.strip(),\n",
        "                'tag_type': entity.attr_type.strip()\n",
        "            })\n",
        "\n",
        "\n",
        "    if all_csv_rows:\n",
        "        df = pd.DataFrame(all_csv_rows)\n",
        "        df_final = df.drop_duplicates(subset=['link', 'tag', 'tag_type']).reset_index(drop=True)\n",
        "\n",
        "        csv_filename = 'tags.csv'\n",
        "        df_final.to_csv(csv_filename, index=False)\n",
        "        print(f\"\\n\\n========================================================\")\n",
        "        print(f\"✅ PIPELINE COMPLETE!\")\n",
        "        print(f\"Generated {len(df_final)} unique tag entries in {csv_filename}\")\n",
        "        print(f\"Generated 10 mermaid_i.md files. (Assuming all new URLs were successfully scraped.)\")\n",
        "        print(f\"========================================================\")\n",
        "    else:\n",
        "        print(\"❌ PIPELINE FAILED: No data was generated for the CSV.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2VQk8Nsn1Hm",
        "outputId": "51e4d8b1-e392-4431-c109-a7d2fd4c113a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing URL 1: https://en.wikipedia.org/wiki/Sustainable_agriculture ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 103 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 103 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 108 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_1.md\n",
            "\n",
            "--- Processing URL 2: https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 62 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 62 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 77 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_2.md\n",
            "\n",
            "--- Processing URL 3: https://en.wikipedia.org/wiki/Climate_change ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 68 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 68 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 73 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_3.md\n",
            "\n",
            "--- Processing URL 4: https://medlineplus.gov/druginformation.html ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 32 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 32 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 36 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_4.md\n",
            "\n",
            "--- Processing URL 5: https://www.fao.org/3/y4671e/y4671e06.htm ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 46 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 46 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 60 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_5.md\n",
            "\n",
            "--- Processing URL 6: https://en.wikipedia.org/wiki/Astrophysics ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 100 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 100 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 96 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_6.md\n",
            "\n",
            "--- Processing URL 7: https://en.wikipedia.org/wiki/Pharmacovigilance ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 50 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 50 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 55 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_7.md\n",
            "\n",
            "--- Processing URL 8: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 36 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 36 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 42 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_8.md\n",
            "\n",
            "--- Processing URL 9: https://www.nps.org.au/consumers/medicine-and-side-effects ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 35 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 35 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 47 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_9.md\n",
            "\n",
            "--- Processing URL 10: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india ---\n",
            "  -> Step 1: Extracting noisy entities...\n",
            "    Extracted 32 initial entities.\n",
            "  -> Step 2: Deduplicating entities with confidence check...\n",
            "    Deduplicated to 32 canonical entities.\n",
            "  -> Step 3: Extracting semantic triples...\n",
            "    Extracted 49 valid triples.\n",
            "  -> Step 4: Generating Mermaid diagram...\n",
            "  -> File saved: mermaid_10.md\n",
            "\n",
            "\n",
            "========================================================\n",
            "✅ PIPELINE COMPLETE!\n",
            "Generated 564 unique tag entries in tags.csv\n",
            "Generated 10 mermaid_i.md files. (Assuming all new URLs were successfully scraped.)\n",
            "========================================================\n"
          ]
        }
      ]
    }
  ]
}