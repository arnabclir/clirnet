{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb20a579",
      "metadata": {
        "id": "bb20a579"
      },
      "source": [
        "# AI Python Developer Assignment – Complete Solution\n",
        "\n",
        "**You can upload this notebook as-is.**\n",
        "\n",
        "What it does:\n",
        "- Uses DSPy with an LLM backend\n",
        "- Scrapes the 10 given URLs\n",
        "- Extracts entities with semantic types\n",
        "- Deduplicates entities with a confidence-based loop\n",
        "- Extracts relational triples\n",
        "- Builds Mermaid graphs per URL\n",
        "- Writes `mermaid_1.md` … `mermaid_10.md`\n",
        "- Writes a `tags.csv` file with columns: `link, tag, tag_type`\n",
        "\n",
        "If the evaluator wants to run it, they only need to set `OPENAI_API_KEY` in the environment (e.g., in Colab secrets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d341e09",
      "metadata": {
        "id": "1d341e09"
      },
      "outputs": [],
      "source": [
        "# 1. Install dependencies (for Colab / runtime)\n",
        "!pip install -q dspy-ai requests beautifulsoup4\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "from typing import List\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import dspy\n",
        "from pydantic import BaseModel, Field\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5827914",
      "metadata": {
        "id": "d5827914"
      },
      "outputs": [],
      "source": [
        "# 2. Configure LLM for DSPy\n",
        "# This uses DSPy's built-in OpenAI integration.\n",
        "# To run it, the evaluator only needs to have OPENAI_API_KEY set in the environment.\n",
        "\n",
        "lm = dspy.OpenAI(model=\"gpt-4o-mini\")\n",
        "dspy.settings.configure(lm=lm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c339d47c",
      "metadata": {
        "id": "c339d47c"
      },
      "outputs": [],
      "source": [
        "# 3. Define DSPy Signatures & Predictors\n",
        "\n",
        "class EntityWithAttr(BaseModel):\n",
        "    entity: str = Field(description=\"the named entity\")\n",
        "    attr_type: str = Field(description=\"semantic type (Drug, Disease, Crop, Process, Measurement, Concept, Location, Person, Organization, Instrument, etc.)\")\n",
        "\n",
        "class ExtractEntities(dspy.Signature):\n",
        "    \"\"\"Extract key entities from a paragraph and type them semantically.\n",
        "    Only return entities that actually appear in the paragraph.\n",
        "    \"\"\"\n",
        "    paragraph: str = dspy.InputField()\n",
        "    entities: List[EntityWithAttr] = dspy.OutputField()\n",
        "\n",
        "\n",
        "class Triple(BaseModel):\n",
        "    source: str = Field(description=\"subject entity string as it appears in text\")\n",
        "    relation: str = Field(description=\"short phrase describing the relationship\")\n",
        "    target: str = Field(description=\"object entity string as it appears in text\")\n",
        "\n",
        "class ExtractTriples(dspy.Signature):\n",
        "    \"\"\"Extract relational triples (source, relation, target) from a paragraph.\n",
        "    Source and target must be entities that appear in the paragraph.\n",
        "    \"\"\"\n",
        "    paragraph: str = dspy.InputField()\n",
        "    triples: List[Triple] = dspy.OutputField()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b37877",
      "metadata": {
        "id": "40b37877"
      },
      "outputs": [],
      "source": [
        "class DeduplicateItems(dspy.Signature):\n",
        "    \"\"\"Deduplicate a list of strings and report confidence.\n",
        "    Use canonical, human-readable forms without merging distinct concepts.\n",
        "    \"\"\"\n",
        "    items: List[str] = dspy.InputField()\n",
        "    deduplicated: List[str] = dspy.OutputField(\n",
        "        description=\"deduplicated list of canonical items\"\n",
        "    )\n",
        "    confidence: float = dspy.OutputField(\n",
        "        description=\"confidence between 0 and 1 that deduplication is correct\"\n",
        "    )\n",
        "\n",
        "extract_entities_predictor = dspy.Predict(ExtractEntities)\n",
        "extract_triples_predictor = dspy.Predict(ExtractTriples)\n",
        "dedup_predictor = dspy.Predict(DeduplicateItems)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b824a1",
      "metadata": {
        "id": "35b824a1"
      },
      "outputs": [],
      "source": [
        "# 4. Deduplication with Confidence Loop\n",
        "\n",
        "def deduplicate_with_lm(items, batch_size=20, target_confidence=0.9, max_tries=4):\n",
        "    \"\"\"Apply LLM-based deduplication on a list of strings with a confidence loop.\n",
        "\n",
        "    Returns a list of canonical strings (deduplicated).\n",
        "    \"\"\"\n",
        "    if not items:\n",
        "        return []\n",
        "\n",
        "    tries = 0\n",
        "    while True:\n",
        "        pred = dedup_predictor(items=list(items))\n",
        "        confidence = float(pred.confidence or 0.0)\n",
        "        print(f\"Dedup confidence: {confidence:.3f}\")\n",
        "\n",
        "        if confidence >= target_confidence:\n",
        "            return [s.strip() for s in pred.deduplicated if s.strip()]\n",
        "\n",
        "        tries += 1\n",
        "        if tries >= max_tries:\n",
        "            # Failsafe: still return what we have\n",
        "            return [s.strip() for s in pred.deduplicated if s.strip()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fce6aa6",
      "metadata": {
        "id": "9fce6aa6"
      },
      "outputs": [],
      "source": [
        "# 5. Mermaid Graph Generation\n",
        "\n",
        "def _clean_mermaid_label(text: str) -> str:\n",
        "    # remove characters that Mermaid dislikes\n",
        "    text = re.sub(r\"[^a-zA-Z0-9_\\-\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    if len(text) > 40:\n",
        "        text = text[:37] + \"...\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def triples_to_mermaid(triples: List[Triple], entity_list: List[str]) -> str:\n",
        "    \"\"\"Convert triples into a Mermaid graph definition.\n",
        "    Only nodes present in entity_list are allowed.\n",
        "    \"\"\"\n",
        "    entity_set = {e.strip().lower() for e in entity_list}\n",
        "\n",
        "    lines = [\"graph LR\"]\n",
        "    edges_seen = set()\n",
        "\n",
        "    for t in triples:\n",
        "        src_raw = t.source.strip()\n",
        "        dst_raw = t.target.strip()\n",
        "        rel_raw = t.relation.strip()\n",
        "\n",
        "        if not src_raw or not dst_raw:\n",
        "            continue\n",
        "\n",
        "        # Only keep edges where both endpoints are in the deduplicated entity set\n",
        "        if src_raw.lower() not in entity_set or dst_raw.lower() not in entity_set:\n",
        "            continue\n",
        "\n",
        "        src = _clean_mermaid_label(src_raw)\n",
        "        dst = _clean_mermaid_label(dst_raw)\n",
        "        label = _clean_mermaid_label(rel_raw)\n",
        "\n",
        "        if not src or not dst or not label:\n",
        "            continue\n",
        "\n",
        "        edge_key = (src, label, dst)\n",
        "        if edge_key in edges_seen:\n",
        "            continue\n",
        "        edges_seen.add(edge_key)\n",
        "\n",
        "        lines.append(f\"    {src} -- {label} --> {dst}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07f1561",
      "metadata": {
        "id": "e07f1561"
      },
      "outputs": [],
      "source": [
        "# 6. Web Scraping Helpers\n",
        "\n",
        "def fetch_url_text(url: str, max_chars: int = 12000) -> str:\n",
        "    \"\"\"Fetch textual content from a URL.\n",
        "    This simple version fetches HTML and concatenates <p> tags.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching: {url}\")\n",
        "    resp = requests.get(url, timeout=20)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # Remove scripts/styles\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    paragraphs = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "    text = \"\\n\".join(paragraphs)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    if len(text) > max_chars:\n",
        "        text = text[:max_chars]\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5532d1d4",
      "metadata": {
        "id": "5532d1d4"
      },
      "outputs": [],
      "source": [
        "# 7. Chunking and Single-URL Processing\n",
        "\n",
        "def chunk_text(text: str, max_tokens_like: int = 1200) -> List[str]:\n",
        "    \"\"\"Roughly chunk text by character length as a token proxy.\"\"\"\n",
        "    chunks = []\n",
        "    current = []\n",
        "    current_len = 0\n",
        "    for sentence in text.split(\". \"):\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "        if current_len + len(sentence) > max_tokens_like:\n",
        "            chunks.append(\". \".join(current).strip())\n",
        "            current = [sentence]\n",
        "            current_len = len(sentence)\n",
        "        else:\n",
        "            current.append(sentence)\n",
        "            current_len += len(sentence)\n",
        "    if current:\n",
        "        chunks.append(\". \".join(current).strip())\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def process_single_url(url: str, index: int):\n",
        "    \"\"\"Process a single URL and return Mermaid text and CSV rows.\n",
        "\n",
        "    Returns:\n",
        "      mermaid_text: string for the Mermaid diagram\n",
        "      csv_rows: list of dicts [{link, tag, tag_type}, ...]\n",
        "    Also writes mermaid_{index}.md to disk.\n",
        "    \"\"\"\n",
        "    raw_text = fetch_url_text(url)\n",
        "    chunks = chunk_text(raw_text)\n",
        "\n",
        "    # 1) Extract entities from all chunks\n",
        "    all_entities = []\n",
        "    for ch in chunks:\n",
        "        if not ch.strip():\n",
        "            continue\n",
        "        pred = extract_entities_predictor(paragraph=ch)\n",
        "        if pred.entities:\n",
        "            all_entities.extend(pred.entities)\n",
        "\n",
        "    # Map: original entity string -> attr_type\n",
        "    entity_to_type = {}\n",
        "    for e in all_entities:\n",
        "        ent = e.entity.strip()\n",
        "        if not ent:\n",
        "            continue\n",
        "        norm = ent.lower()\n",
        "        if norm not in entity_to_type:\n",
        "            entity_to_type[norm] = e.attr_type.strip() if e.attr_type else \"Unknown\"\n",
        "\n",
        "    # Deduplicate\n",
        "    unique_raw_entities = list(entity_to_type.keys())  # lowercase strings\n",
        "    dedup_canonical = deduplicate_with_lm(unique_raw_entities, target_confidence=0.9)\n",
        "    dedup_canonical_norm = [e.lower() for e in dedup_canonical]\n",
        "\n",
        "    # Build final entities list (canonical string + type)\n",
        "    final_entities = []\n",
        "    for norm_ent in dedup_canonical_norm:\n",
        "        if norm_ent in entity_to_type:\n",
        "            attr_type = entity_to_type[norm_ent]\n",
        "            tag = norm_ent\n",
        "        else:\n",
        "            attr_type = \"Unknown\"\n",
        "            tag = norm_ent\n",
        "        final_entities.append((tag, attr_type))\n",
        "\n",
        "    # 2) Extract triples\n",
        "    all_triples = []\n",
        "    for ch in chunks:\n",
        "        if not ch.strip():\n",
        "            continue\n",
        "        pred = extract_triples_predictor(paragraph=ch)\n",
        "        if pred.triples:\n",
        "            all_triples.extend(pred.triples)\n",
        "\n",
        "    # 3) Generate Mermaid text\n",
        "    canonical_surface_forms = [e[0] for e in final_entities]\n",
        "    mermaid_text = triples_to_mermaid(all_triples, canonical_surface_forms)\n",
        "\n",
        "    # 4) Write Mermaid file\n",
        "    filename = f\"mermaid_{index}.md\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"```mermaid\\n\")\n",
        "        f.write(mermaid_text)\n",
        "        f.write(\"\\n```\")\n",
        "    print(f\"Wrote {filename}\")\n",
        "\n",
        "    # 5) Prepare CSV rows (no duplicates per URL)\n",
        "    csv_rows = []\n",
        "    seen_tags = set()\n",
        "    for tag, tag_type in final_entities:\n",
        "        clean_tag = tag.strip()\n",
        "        if not clean_tag:\n",
        "            continue\n",
        "        if clean_tag.lower() in seen_tags:\n",
        "            continue\n",
        "        seen_tags.add(clean_tag.lower())\n",
        "        csv_rows.append(\n",
        "            {\n",
        "                \"link\": url,\n",
        "                \"tag\": clean_tag,\n",
        "                \"tag_type\": tag_type or \"Unknown\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return mermaid_text, csv_rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8666f27c",
      "metadata": {
        "id": "8666f27c"
      },
      "outputs": [],
      "source": [
        "# 8. Run for All URLs and Export tags.csv\n",
        "\n",
        "URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
        "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\",\n",
        "]\n",
        "\n",
        "all_csv_rows = []\n",
        "\n",
        "for i, url in enumerate(URLS, start=1):\n",
        "    try:\n",
        "        mermaid_text, csv_rows = process_single_url(url, i)\n",
        "        all_csv_rows.extend(csv_rows)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "# Write tags.csv\n",
        "csv_filename = \"tags.csv\"\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"link\", \"tag\", \"tag_type\"])\n",
        "    writer.writeheader()\n",
        "    for row in all_csv_rows:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Wrote {csv_filename} with {len(all_csv_rows)} rows.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}