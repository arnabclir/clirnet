{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q beautifulsoup4 readability-lxml rapidfuzz pandas spacy openai\n",
        "\n",
        "import sys, subprocess\n",
        "subprocess.run([sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
        "print('Install complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNs-Ex9_D5ew",
        "outputId": "f23d0e5f-cb65-4b73-bbe6-24701a84668b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m2.3/3.2 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstall complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edyzkfzbDMcI",
        "outputId": "6931d5cb-1385-440a-fd3d-2c7263dd7b8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: readability-lxml in /usr/local/lib/python3.12/dist-packages (0.8.4.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (5.2.0)\n",
            "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (6.0.2)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (1.3.0)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.12/dist-packages (from lxml[html_clean]->readability-lxml) (0.4.3)\n",
            "Configuration set. Output directory: /content/dspy_beginner_outputs\n"
          ]
        }
      ],
      "source": [
        "import os, time, re, math, json\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document\n",
        "import spacy\n",
        "from rapidfuzz import fuzz\n",
        "import pandas as pd\n",
        "\n",
        "# Install missing package\n",
        "!pip install readability-lxml\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "OUTPUT_DIR = '/content/dspy_beginner_outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "RATE_LIMIT_SECONDS = 1.5\n",
        "\n",
        "# URLs to process (assignment)\n",
        "URLS = [\n",
        " \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        " \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        " \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        " \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        " \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        " \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        " \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        " \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-plabets\",\n",
        " \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        " \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
        "]\n",
        "\n",
        "print('Configuration set. Output directory:', OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# robots.txt check and robust fetch\n",
        "from urllib import robotparser\n",
        "\n",
        "def can_fetch(url, user_agent='*'):\n",
        "    parsed = urlparse(url)\n",
        "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
        "    rp = robotparser.RobotFileParser()\n",
        "    try:\n",
        "        rp.set_url(robots_url)\n",
        "        rp.read()\n",
        "        return rp.can_fetch(user_agent, url)\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "def fetch_url_text(url):\n",
        "    if not can_fetch(url):\n",
        "        return (f\"Fetching disallowed by robots.txt: {url}\", False)\n",
        "    headers = {\"User-Agent\": \"DSPyBeginnerBot/1.0 (+example)\"}\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, timeout=15)\n",
        "        if r.status_code != 200:\n",
        "            return (f\"HTTP {r.status_code}: could not fetch {url}\", False)\n",
        "        doc = Document(r.text)\n",
        "        title = doc.short_title()\n",
        "        summary_html = doc.summary()\n",
        "        soup = BeautifulSoup(summary_html, 'html.parser')\n",
        "        text = soup.get_text(separator='\\n')\n",
        "        if len(text.strip()) < 200:\n",
        "            soup2 = BeautifulSoup(r.text, 'html.parser')\n",
        "            ps = soup2.find_all('p')\n",
        "            text = '\\n'.join([p.get_text() for p in ps])\n",
        "        clean = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        return (f\"TITLE: {title}\\n\\n{clean}\", True)\n",
        "    except Exception as e:\n",
        "        return (f\"Error fetching {url}: {e}\", False)\n"
      ],
      "metadata": {
        "id": "XwHX47WWDfzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beginner-friendly extractor using spaCy (named entities + noun chunks)\n",
        "CATEGORY_MAP = {\n",
        "    'PERSON': 'Person',\n",
        "    'ORG': 'Organization',\n",
        "    'GPE': 'Location',\n",
        "    'LOC': 'Location',\n",
        "    'DATE': 'Measurement',\n",
        "    'NORP': 'Concept',\n",
        "    'PRODUCT': 'Concept',\n",
        "    'EVENT': 'Concept',\n",
        "    'WORK_OF_ART': 'Concept',\n",
        "    'LAW': 'Concept',\n",
        "    'LANGUAGE': 'Concept',\n",
        "}\n",
        "\n",
        "def extract_entities_spacy(text, max_entities=200):\n",
        "    doc = nlp(text)\n",
        "    ents = set()\n",
        "    for e in doc.ents:\n",
        "        label = CATEGORY_MAP.get(e.label_, 'Other')\n",
        "        ents.add((e.text.strip(), label))\n",
        "    for chunk in doc.noun_chunks:\n",
        "        text_chunk = chunk.text.strip()\n",
        "        if len(text_chunk) > 2 and len(text_chunk.split()) <= 5:\n",
        "            ents.add((text_chunk, 'Concept'))\n",
        "    ents_list = list(ents)[:max_entities]\n",
        "    ents_list = [(re.sub('\\s+', ' ', e).strip(), t) for e,t in ents_list]\n",
        "    return [{'entity': e, 'attr_type': t} for e,t in ents_list]\n",
        "\n",
        "# Quick test (optional)\n",
        "# print(extract_entities_spacy('Sustainable agriculture improves soil health and nitrogen uptake.'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTPxBiFHEWlN",
        "outputId": "4e0d9ace-c43c-4bb5-ac44-0c7ca581c3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-2107851208.py:27: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  ents_list = [(re.sub('\\s+', ' ', e).strip(), t) for e,t in ents_list]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# LLM integration:\n",
        "# Option C behavior: prefer LongCat if configured, else try OpenAI, else fallback to spaCy.\n",
        "#\n",
        "# For LongCat: a placeholder is provided — paste your LongCat request code where indicated.\n",
        "# For OpenAI: this cell implements a working call if OPENAI_API_KEY is set.\n",
        "\n",
        "OPENAI_KEY = os.getenv('OPENAI_API_KEY')\n",
        "LONGCAT_KEY = os.getenv('LONGCAT_API_KEY')\n",
        "\n",
        "USE_OPENAI = bool(OPENAI_KEY)\n",
        "USE_LONGCAT = bool(LONGCAT_KEY)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    import openai\n",
        "    openai.api_key = OPENAI_KEY\n",
        "    print('OpenAI key detected: will use OpenAI if requested.')\n",
        "else:\n",
        "    print('No OpenAI key detected.')\n",
        "\n",
        "if USE_LONGCAT:\n",
        "    print('LongCat key detected: please paste LongCat API call in the placeholder cell if you want to use LongCat.')\n",
        "else:\n",
        "    print('No LongCat key detected.')\n",
        "\n",
        "def llm_extract_entities_openai(paragraph, max_tokens=600):\n",
        "    \"\"\"Call OpenAI to extract entities; returns list of {'entity','attr_type'} or [] on failure.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are a precise extractor. Given the paragraph below, return JSON only with:\n",
        "{{\"paragraph\": \"<the paragraph string>\", \"entities\": [{{\"entity\":\"<exact substring>\", \"attr_type\":\"<semantic type>\"}}, ...]}}\n",
        "Allowed attr_type examples: Crop, Process, Measurement, Drug, Disease, Concept, Person, Organization, Location, Technique, Other.\n",
        "Return only JSON and no extra text.\n",
        "\n",
        "Paragraph:\n",
        "{paragraph}\n",
        "\"\"\"\n",
        "    try:\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model='gpt-4o-mini',\n",
        "            messages=[{'role':'user','content':prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0\n",
        "        )\n",
        "        text = resp['choices'][0]['message']['content']\n",
        "        # Try to parse JSON from the response\n",
        "        j = json.loads(text)\n",
        "        ents = j.get('entities', [])\n",
        "        # validate shape simply\n",
        "        good = []\n",
        "        for e in ents:\n",
        "            if isinstance(e, dict) and 'entity' in e and 'attr_type' in e:\n",
        "                good.append({'entity': e['entity'].strip(), 'attr_type': e['attr_type'].strip()})\n",
        "        return good\n",
        "    except Exception as e:\n",
        "        print('OpenAI extraction error:', e)\n",
        "        return []\n",
        "\n",
        "# Placeholder function for LongCat: if you have LONGCAT_KEY, modify this function to call LongCat's API.\n",
        "def llm_extract_entities_longcat(paragraph):\n",
        "    # Example: use requests.post to LongCat endpoint with LONGCAT_KEY in Authorization header.\n",
        "    # The exact request format depends on LongCat's API. Paste your working call here.\n",
        "    print('LongCat extraction placeholder — please implement your LongCat call in this function.')\n",
        "    return []\n",
        "\n",
        "# Master extractor: tries LongCat -> OpenAI -> spaCy fallback\n",
        "def extract_entities_smart(paragraph):\n",
        "    if USE_LONGCAT:\n",
        "        ents = llm_extract_entities_longcat(paragraph)\n",
        "        if ents:\n",
        "            return ents\n",
        "    if USE_OPENAI:\n",
        "        ents = llm_extract_entities_openai(paragraph)\n",
        "        if ents:\n",
        "            return ents\n",
        "    # fallback\n",
        "    return extract_entities_spacy(paragraph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I19viuCAEdWl",
        "outputId": "f12be7e2-d263-4e89-d0f7-c50197892405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No OpenAI key detected.\n",
            "No LongCat key detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deduplication with fuzzy grouping\n",
        "def fuzzy_group(entities, threshold=88):\n",
        "    remaining = set(entities)\n",
        "    clusters = []\n",
        "    while remaining:\n",
        "        e = remaining.pop()\n",
        "        cluster = [e]\n",
        "        for other in list(remaining):\n",
        "            score = fuzz.token_sort_ratio(e, other)\n",
        "            if score >= threshold:\n",
        "                cluster.append(other)\n",
        "                remaining.remove(other)\n",
        "        clusters.append(cluster)\n",
        "    return clusters\n",
        "\n",
        "def canonical_name(cluster):\n",
        "    return sorted(cluster, key=lambda s: (-len(s), s))[0]\n"
      ],
      "metadata": {
        "id": "sfPEk9q0EiRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mermaid helpers\n",
        "def triples_to_mermaid(triples, entity_list, max_label_len=40):\n",
        "    entity_set = {e.strip().lower() for e in entity_list}\n",
        "    def _clean(s):\n",
        "        return s.replace('\"', \"'\").strip()\n",
        "    lines = ['```mermaid', 'graph TD']\n",
        "    for src, lbl, dst in triples:\n",
        "        if src.strip().lower() in entity_set and dst.strip().lower() in entity_set:\n",
        "            label_trim = (lbl[:max_label_len] + '...') if len(lbl) > max_label_len else lbl\n",
        "            lines.append(f'  \"{_clean(src)}\" -- \"{_clean(label_trim)}\" --> \"{_clean(dst)}\"')\n",
        "    lines.append('```')\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def empty_mermaid(entities):\n",
        "    lines = ['```mermaid', 'graph TD']\n",
        "    for e in entities:\n",
        "        lines.append(f'  \"{e}\"')\n",
        "    lines.append('```')\n",
        "    return '\\n'.join(lines)\n"
      ],
      "metadata": {
        "id": "GPTSFOMgEssK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orchestrator: process all URLs\n",
        "all_tags = []\n",
        "\n",
        "for idx, url in enumerate(URLS, start=1):\n",
        "    print(f\"\\n--- Processing ({idx}/10): {url}\")\n",
        "\n",
        "    text, ok = fetch_url_text(url)\n",
        "    raw_fn = os.path.join(OUTPUT_DIR, f'raw_{idx}.txt')\n",
        "    with open(raw_fn, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "    if not ok:\n",
        "        print(f\"Warning fetching {url}: continuing with available text snippet.\")\n",
        "\n",
        "    # Chunk text into paragraphs for extraction\n",
        "    paragraphs = [p for p in text.split('\\n\\n') if len(p.strip()) > 30]\n",
        "    # Extract entities for each paragraph using the smart extractor\n",
        "    extracted = []\n",
        "    for p in paragraphs[:10]:  # limit to first 10 paragraphs to save tokens/time\n",
        "        ents = extract_entities_smart(p)\n",
        "        for e in ents:\n",
        "            extracted.append(e)\n",
        "    print(f\"Extracted (raw) {len(extracted)} entity mentions (may contain duplicates)\")\n",
        "\n",
        "    # Deduplicate\n",
        "    names = [e['entity'] for e in extracted]\n",
        "    clusters = fuzzy_group(names, threshold=88)\n",
        "    canonical_map = {}\n",
        "    for cluster in clusters:\n",
        "        canon = canonical_name(cluster)\n",
        "        for member in cluster:\n",
        "            canonical_map[member] = canon\n",
        "\n",
        "    dedup = {}\n",
        "    for it in extracted:\n",
        "        key = canonical_map.get(it['entity'], it['entity'])\n",
        "        if key not in dedup:\n",
        "            dedup[key] = {'entity': key, 'attr_type': it.get('attr_type', 'Other'), 'members':[it['entity']]}\n",
        "        else:\n",
        "            dedup[key]['members'].append(it['entity'])\n",
        "            if dedup[key]['attr_type'] != it.get('attr_type', 'Other'):\n",
        "                dedup[key]['attr_type'] = 'Multiple'\n",
        "\n",
        "    dedup_list = list(dedup.values())\n",
        "    print(f\"Deduplicated to {len(dedup_list)} entities\")\n",
        "\n",
        "    # Build simple co-occurrence triples\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    entity_names = [d['entity'] for d in dedup_list]\n",
        "    triples = set()\n",
        "    for s in sentences:\n",
        "        present = [e for e in entity_names if re.search(re.escape(e), s, re.IGNORECASE)]\n",
        "        for i in range(len(present)):\n",
        "            for j in range(i+1, len(present)):\n",
        "                triples.add((present[i], 'co-occurs', present[j]))\n",
        "    triples = list(triples)\n",
        "\n",
        "    if triples:\n",
        "        mermaid_text = triples_to_mermaid(triples, entity_names)\n",
        "    else:\n",
        "        mermaid_text = empty_mermaid(entity_names)\n",
        "\n",
        "    mermaid_fn = os.path.join(OUTPUT_DIR, f'mermaid_{idx}.md')\n",
        "    with open(mermaid_fn, 'w', encoding='utf-8') as f:\n",
        "        f.write(mermaid_text)\n",
        "    print(f\"Wrote {mermaid_fn}\")\n",
        "\n",
        "    seen = set()\n",
        "    for d in dedup_list:\n",
        "        tag = d['entity']\n",
        "        tag_type = d['attr_type']\n",
        "        if tag.lower() not in seen:\n",
        "            all_tags.append({'link': url, 'tag': tag, 'tag_type': tag_type})\n",
        "            seen.add(tag.lower())\n",
        "\n",
        "# Save tags.csv\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(all_tags)\n",
        "df.to_csv(os.path.join(OUTPUT_DIR, 'tags.csv'), index=False)\n",
        "print('\\nDone. Outputs in', OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIj1QYq0Eywz",
        "outputId": "916a0c9c-8ba6-48cf-b9a7-b9117b3bdbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing (1/10): https://en.wikipedia.org/wiki/Sustainable_agriculture\n",
            "Warning fetching https://en.wikipedia.org/wiki/Sustainable_agriculture: continuing with available text snippet.\n",
            "Extracted (raw) 0 entity mentions (may contain duplicates)\n",
            "Deduplicated to 0 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_1.md\n",
            "\n",
            "--- Processing (2/10): https://www.nature.com/articles/d41586-025-03353-5\n",
            "Extracted (raw) 82 entity mentions (may contain duplicates)\n",
            "Deduplicated to 75 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_2.md\n",
            "\n",
            "--- Processing (3/10): https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "Warning fetching https://www.sciencedirect.com/science/article/pii/S1043661820315152: continuing with available text snippet.\n",
            "Extracted (raw) 0 entity mentions (may contain duplicates)\n",
            "Deduplicated to 0 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_3.md\n",
            "\n",
            "--- Processing (4/10): https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\n",
            "Warning fetching https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/: continuing with available text snippet.\n",
            "Extracted (raw) 1 entity mentions (may contain duplicates)\n",
            "Deduplicated to 1 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_4.md\n",
            "\n",
            "--- Processing (5/10): https://www.fao.org/3/y4671e/y4671e06.htm\n",
            "Extracted (raw) 158 entity mentions (may contain duplicates)\n",
            "Deduplicated to 129 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_5.md\n",
            "\n",
            "--- Processing (6/10): https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
            "Warning fetching https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria: continuing with available text snippet.\n",
            "Extracted (raw) 1 entity mentions (may contain duplicates)\n",
            "Deduplicated to 1 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_6.md\n",
            "\n",
            "--- Processing (7/10): https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "Warning fetching https://www.sciencedirect.com/science/article/pii/S0378378220307088: continuing with available text snippet.\n",
            "Extracted (raw) 0 entity mentions (may contain duplicates)\n",
            "Deduplicated to 0 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_7.md\n",
            "\n",
            "--- Processing (8/10): https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-plabets\n",
            "Warning fetching https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-plabets: continuing with available text snippet.\n",
            "Extracted (raw) 2 entity mentions (may contain duplicates)\n",
            "Deduplicated to 2 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_8.md\n",
            "\n",
            "--- Processing (9/10): https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
            "Warning fetching https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7: continuing with available text snippet.\n",
            "Extracted (raw) 2 entity mentions (may contain duplicates)\n",
            "Deduplicated to 2 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_9.md\n",
            "\n",
            "--- Processing (10/10): https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\n",
            "Extracted (raw) 287 entity mentions (may contain duplicates)\n",
            "Deduplicated to 238 entities\n",
            "Wrote /content/dspy_beginner_outputs/mermaid_10.md\n",
            "\n",
            "Done. Outputs in /content/dspy_beginner_outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List produced files\n",
        "print('Files in output directory:')\n",
        "import os\n",
        "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
        "    print('-', f)\n",
        "print('\\nTo view a mermaid diagram, open mermaid_X.md and paste its contents into https://mermaid.live to visualize.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXj40QAbE31q",
        "outputId": "a2c712a5-0b83-41c7-ed3a-5185d4f4a649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in output directory:\n",
            "- mermaid_1.md\n",
            "- mermaid_10.md\n",
            "- mermaid_2.md\n",
            "- mermaid_3.md\n",
            "- mermaid_4.md\n",
            "- mermaid_5.md\n",
            "- mermaid_6.md\n",
            "- mermaid_7.md\n",
            "- mermaid_8.md\n",
            "- mermaid_9.md\n",
            "- raw_1.txt\n",
            "- raw_10.txt\n",
            "- raw_2.txt\n",
            "- raw_3.txt\n",
            "- raw_4.txt\n",
            "- raw_5.txt\n",
            "- raw_6.txt\n",
            "- raw_7.txt\n",
            "- raw_8.txt\n",
            "- raw_9.txt\n",
            "- tags.csv\n",
            "\n",
            "To view a mermaid diagram, open mermaid_X.md and paste its contents into https://mermaid.live to visualize.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifOiJcGZFTpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}