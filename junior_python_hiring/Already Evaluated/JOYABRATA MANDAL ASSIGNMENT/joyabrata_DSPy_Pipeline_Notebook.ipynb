{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DSPy Pipeline for Structuring Unstructured Data\n",
                "\n",
                "This notebook demonstrates how to use DSPy to extract structured entities and relationships from web content.\n",
                "\n",
                "## Key Concepts\n",
                "1. **Entity Extraction**: Using Pydantic models to force structured LLM outputs\n",
                "2. **Confidence-based Deduplication**: Retry loops until LLM confidence meets threshold\n",
                "3. **Mermaid Graph Generation**: Visualizing relationships with strict entity validation\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q dspy-ai pydantic requests beautifulsoup4 html2text pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import os\n",
                "import csv\n",
                "import re\n",
                "import dspy\n",
                "import requests\n",
                "import pandas as pd\n",
                "from bs4 import BeautifulSoup\n",
                "import html2text\n",
                "from pydantic import BaseModel, Field\n",
                "from typing import List, Dict, Tuple, Optional, Set\n",
                "from IPython.display import display, Markdown\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configure Longcat API\n",
                "\n",
                "Get your free API key from [Longcat API Platform](https://api.longcat.chat)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your Longcat API key here\n",
                "LONGCAT_API_KEY = \"your-api-key-here\"  # Replace with your actual key\n",
                "\n",
                "\n",
                "# Configure DSPy with Longcat API\n",
                "lm = dspy.LM(\n",
                "    model=\"openai/LongCat-Flash-Chat\",\n",
                "    api_base=\"https://api.longcat.chat/openai/v1\",\n",
                "    api_key=LONGCAT_API_KEY,\n",
                "    temperature=0.7,\n",
                "    max_tokens=4096\n",
                ")\n",
                "dspy.configure(lm=lm)\n",
                "print(\"✓ DSPy configured with Longcat API\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Define Pydantic Models & DSPy Signatures\n",
                "\n",
                "These enforce structured outputs from the LLM - no more regex parsing!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pydantic Models for Structured Output\n",
                "class EntityWithAttr(BaseModel):\n",
                "    \"\"\"An entity with its semantic type.\"\"\"\n",
                "    entity: str = Field(description=\"The named entity\")\n",
                "    attr_type: str = Field(description=\"Semantic type (e.g., Concept, Process, Drug)\")\n",
                "\n",
                "class Triple(BaseModel):\n",
                "    \"\"\"A relationship triple between entities.\"\"\"\n",
                "    source: str = Field(description=\"Source entity\")\n",
                "    relation: str = Field(description=\"Relationship type\")\n",
                "    target: str = Field(description=\"Target entity\")\n",
                "\n",
                "# DSPy Signatures\n",
                "class ExtractEntities(dspy.Signature):\n",
                "    \"\"\"Extract named entities from text with their semantic types.\"\"\"\n",
                "    paragraph: str = dspy.InputField(desc=\"Text to extract entities from\")\n",
                "    entities: List[EntityWithAttr] = dspy.OutputField(desc=\"Extracted entities\")\n",
                "\n",
                "class ExtractTriples(dspy.Signature):\n",
                "    \"\"\"Extract relationship triples using only provided entities.\"\"\"\n",
                "    paragraph: str = dspy.InputField(desc=\"Text to extract relationships from\")\n",
                "    entity_list: List[str] = dspy.InputField(desc=\"Valid entities to use\")\n",
                "    triples: List[Triple] = dspy.OutputField(desc=\"Relationship triples\")\n",
                "\n",
                "class DeduplicateEntities(dspy.Signature):\n",
                "    \"\"\"Deduplicate entities by grouping synonyms/variants.\"\"\"\n",
                "    items: List[str] = dspy.InputField(desc=\"Entities to deduplicate\")\n",
                "    deduplicated: List[str] = dspy.OutputField(desc=\"Canonical entity names\")\n",
                "    confidence: float = dspy.OutputField(desc=\"Confidence score 0-1\")\n",
                "\n",
                "print(\"✓ Models and signatures defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. URL Scraping Module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_url(url: str, timeout: int = 30) -> Optional[str]:\n",
                "    \"\"\"Scrape and clean text content from a URL.\"\"\"\n",
                "    headers = {\n",
                "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
                "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        response = requests.get(url, headers=headers, timeout=timeout)\n",
                "        response.raise_for_status()\n",
                "        \n",
                "        soup = BeautifulSoup(response.content, 'html.parser')\n",
                "        \n",
                "        # Remove non-content elements\n",
                "        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
                "            element.decompose()\n",
                "        \n",
                "        # Find main content\n",
                "        main_content = None\n",
                "        for selector in ['article', 'main', '.content', '#content', '.article-body']:\n",
                "            main_content = soup.select_one(selector) if selector.startswith('.') or selector.startswith('#') else soup.find(selector)\n",
                "            if main_content:\n",
                "                break\n",
                "        \n",
                "        if not main_content:\n",
                "            main_content = soup.find('body')\n",
                "        \n",
                "        if not main_content:\n",
                "            return None\n",
                "        \n",
                "        # Convert to text\n",
                "        h = html2text.HTML2Text()\n",
                "        h.ignore_links = True\n",
                "        h.ignore_images = True\n",
                "        h.body_width = 0\n",
                "        \n",
                "        text = h.handle(str(main_content))\n",
                "        \n",
                "        # Clean text\n",
                "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
                "        text = re.sub(r' {2,}', ' ', text)\n",
                "        text = re.sub(r'\\[.*?\\]', '', text)\n",
                "        \n",
                "        # Remove short lines\n",
                "        lines = [l for l in text.split('\\n') if len(l.strip()) > 20 or l.strip() == '']\n",
                "        return '\\n'.join(lines).strip()\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error scraping {url}: {e}\")\n",
                "        return None\n",
                "\n",
                "def chunk_text(text: str, max_size: int = 2500) -> List[str]:\n",
                "    \"\"\"Split text into chunks for processing.\"\"\"\n",
                "    paragraphs = text.split('\\n\\n')\n",
                "    chunks = []\n",
                "    current = \"\"\n",
                "    \n",
                "    for para in paragraphs:\n",
                "        if len(current) + len(para) + 2 <= max_size:\n",
                "            current += para + \"\\n\\n\"\n",
                "        else:\n",
                "            if current:\n",
                "                chunks.append(current.strip())\n",
                "            current = para + \"\\n\\n\"\n",
                "    \n",
                "    if current:\n",
                "        chunks.append(current.strip())\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "print(\"✓ Scraping functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Entity Extraction & Deduplication"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize predictors (using dspy.Predict - current API)\n",
                "entity_predictor = dspy.Predict(ExtractEntities)\n",
                "triple_predictor = dspy.Predict(ExtractTriples)\n",
                "dedup_predictor = dspy.Predict(DeduplicateEntities)\n",
                "\n",
                "def extract_entities(text: str) -> List[EntityWithAttr]:\n",
                "    \"\"\"Extract entities from text.\"\"\"\n",
                "    try:\n",
                "        result = entity_predictor(paragraph=text)\n",
                "        return result.entities\n",
                "    except Exception as e:\n",
                "        print(f\"Entity extraction error: {e}\")\n",
                "        return []\n",
                "\n",
                "def extract_triples(text: str, entity_list: List[str]) -> List[Triple]:\n",
                "    \"\"\"Extract relationship triples from text.\"\"\"\n",
                "    try:\n",
                "        result = triple_predictor(paragraph=text, entity_list=entity_list)\n",
                "        \n",
                "        # Filter to valid entities only\n",
                "        valid_entities = {e.lower().strip() for e in entity_list}\n",
                "        valid_triples = [\n",
                "            t for t in result.triples\n",
                "            if t.source.lower().strip() in valid_entities and t.target.lower().strip() in valid_entities\n",
                "        ]\n",
                "        return valid_triples\n",
                "    except Exception as e:\n",
                "        print(f\"Triple extraction error: {e}\")\n",
                "        return []\n",
                "\n",
                "def deduplicate_with_confidence(items: List[str], target_confidence: float = 0.85, max_retries: int = 3):\n",
                "    \"\"\"\n",
                "    Deduplicate with confidence loop - keeps retrying until confidence >= target.\n",
                "    This is a key safety feature: LLMs hallucinate, so we validate!\n",
                "    \"\"\"\n",
                "    if not items:\n",
                "        return [], {}\n",
                "    \n",
                "    # Remove exact duplicates first\n",
                "    seen = {}\n",
                "    unique_items = []\n",
                "    for item in items:\n",
                "        key = item.lower().strip()\n",
                "        if key not in seen:\n",
                "            seen[key] = item\n",
                "            unique_items.append(item)\n",
                "    \n",
                "    best_result = None\n",
                "    best_confidence = 0.0\n",
                "    \n",
                "    for attempt in range(max_retries):\n",
                "        try:\n",
                "            pred = dedup_predictor(items=unique_items)\n",
                "            confidence = float(pred.confidence)\n",
                "            \n",
                "            if confidence > best_confidence:\n",
                "                best_confidence = confidence\n",
                "                best_result = pred\n",
                "            \n",
                "            if confidence >= target_confidence:\n",
                "                print(f\"  ✓ Deduplication confidence: {confidence:.2f}\")\n",
                "                break\n",
                "        except Exception as e:\n",
                "            print(f\"  Dedup attempt {attempt+1} failed: {e}\")\n",
                "    \n",
                "    if best_result is None:\n",
                "        return unique_items, {item: item for item in unique_items}\n",
                "    \n",
                "    # Build mapping\n",
                "    mapping = {}\n",
                "    dedup_lower = {d.lower().strip(): d for d in best_result.deduplicated}\n",
                "    \n",
                "    for item in unique_items:\n",
                "        item_lower = item.lower().strip()\n",
                "        if item_lower in dedup_lower:\n",
                "            mapping[item] = dedup_lower[item_lower]\n",
                "        else:\n",
                "            mapping[item] = item\n",
                "    \n",
                "    return best_result.deduplicated, mapping\n",
                "\n",
                "print(\"✓ Extraction and deduplication functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Mermaid Diagram Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_label(label: str, max_len: int = 40) -> str:\n",
                "    \"\"\"Clean label for Mermaid syntax.\"\"\"\n",
                "    label = re.sub(r'[\"\\'\\[\\]{}()<>|\\\\]', '', label)\n",
                "    label = label.replace('&', 'and').replace('#', '').replace('--', '-')\n",
                "    return label[:max_len-3] + \"...\" if len(label) > max_len else label.strip()\n",
                "\n",
                "def clean_node_id(name: str) -> str:\n",
                "    \"\"\"Convert name to valid Mermaid node ID.\"\"\"\n",
                "    node_id = re.sub(r'[^a-zA-Z0-9]', '_', name.lower())\n",
                "    node_id = re.sub(r'_+', '_', node_id).strip('_')\n",
                "    if node_id and not node_id[0].isalpha():\n",
                "        node_id = 'n_' + node_id\n",
                "    return node_id or 'unknown'\n",
                "\n",
                "def triples_to_mermaid(triples: List[Triple], entities: List[EntityWithAttr], title: str = \"\") -> str:\n",
                "    \"\"\"\n",
                "    Convert triples to Mermaid graph.\n",
                "    Only allows entities from deduplicated list as nodes - prevents garbage!\n",
                "    \"\"\"\n",
                "    entity_set = {e.entity.strip().lower() for e in entities}\n",
                "    entity_display = {e.entity.strip().lower(): e.entity for e in entities}\n",
                "    \n",
                "    lines = []\n",
                "    if title:\n",
                "        lines.extend([\"---\", f\"title: {clean_label(title, 60)}\", \"---\"])\n",
                "    lines.append(\"graph TD\")\n",
                "    \n",
                "    edges = []\n",
                "    for t in triples:\n",
                "        src_lower = t.source.strip().lower()\n",
                "        dst_lower = t.target.strip().lower()\n",
                "        \n",
                "        if src_lower not in entity_set or dst_lower not in entity_set:\n",
                "            continue\n",
                "        if src_lower == dst_lower:\n",
                "            continue\n",
                "        \n",
                "        src_disp = entity_display.get(src_lower, t.source)\n",
                "        dst_disp = entity_display.get(dst_lower, t.target)\n",
                "        \n",
                "        edge = f\"    {clean_node_id(src_disp)}[\\\"{clean_label(src_disp)}\\\"] -- {clean_label(t.relation)} --> {clean_node_id(dst_disp)}[\\\"{clean_label(dst_disp)}\\\"]\"\n",
                "        edges.append(edge)\n",
                "    \n",
                "    lines.extend(list(dict.fromkeys(edges)))\n",
                "    return \"\\n\".join(lines)\n",
                "\n",
                "print(\"✓ Mermaid generation functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Define URLs to Process"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ASSIGNMENT_URLS = [\n",
                "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
                "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
                "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
                "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
                "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
                "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
                "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
                "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
                "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
                "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\",\n",
                "]\n",
                "\n",
                "print(f\"✓ {len(ASSIGNMENT_URLS)} URLs to process\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Run the Full Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Process all URLs\n",
                "all_results = []\n",
                "\n",
                "for i, url in enumerate(ASSIGNMENT_URLS, 1):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Processing URL {i}/10: {url[:60]}...\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    try:\n",
                "        # Scrape\n",
                "        content = scrape_url(url)\n",
                "        if not content or len(content) < 100:\n",
                "            print(f\"  ✗ Could not scrape content\")\n",
                "            all_results.append({'url': url, 'entities': [], 'triples': []})\n",
                "            continue\n",
                "        print(f\"  ✓ Scraped {len(content)} chars\")\n",
                "        \n",
                "        # Chunk\n",
                "        chunks = chunk_text(content)[:5]\n",
                "        print(f\"  ✓ Processing {len(chunks)} chunks\")\n",
                "        \n",
                "        # Extract entities\n",
                "        all_entities = []\n",
                "        for chunk in chunks:\n",
                "            entities = extract_entities(chunk)\n",
                "            all_entities.extend(entities)\n",
                "        print(f\"  ✓ Extracted {len(all_entities)} raw entities\")\n",
                "        \n",
                "        # Deduplicate\n",
                "        entity_names = [e.entity for e in all_entities]\n",
                "        dedup_names, mapping = deduplicate_with_confidence(entity_names)\n",
                "        \n",
                "        # Create deduplicated entity objects\n",
                "        type_votes = {}\n",
                "        for e in all_entities:\n",
                "            canon = mapping.get(e.entity, e.entity)\n",
                "            if canon not in type_votes:\n",
                "                type_votes[canon] = {}\n",
                "            type_votes[canon][e.attr_type] = type_votes[canon].get(e.attr_type, 0) + 1\n",
                "        \n",
                "        dedup_entities = []\n",
                "        for name in dedup_names:\n",
                "            if name in type_votes:\n",
                "                best_type = max(type_votes[name].items(), key=lambda x: x[1])[0]\n",
                "            else:\n",
                "                best_type = \"Concept\"\n",
                "            dedup_entities.append(EntityWithAttr(entity=name, attr_type=best_type))\n",
                "        print(f\"  ✓ Deduplicated to {len(dedup_entities)} entities\")\n",
                "        \n",
                "        # Extract triples\n",
                "        all_triples = []\n",
                "        for chunk in chunks[:3]:  # Limit for API\n",
                "            triples = extract_triples(chunk, dedup_names)\n",
                "            all_triples.extend(triples)\n",
                "        \n",
                "        # Map and deduplicate triples\n",
                "        mapped_triples = []\n",
                "        seen = set()\n",
                "        for t in all_triples:\n",
                "            key = (t.source.lower(), t.relation.lower(), t.target.lower())\n",
                "            if key not in seen:\n",
                "                seen.add(key)\n",
                "                mapped_triples.append(t)\n",
                "        print(f\"  ✓ Extracted {len(mapped_triples)} unique triples\")\n",
                "        \n",
                "        all_results.append({\n",
                "            'url': url,\n",
                "            'entities': dedup_entities,\n",
                "            'triples': mapped_triples\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ✗ Error: {e}\")\n",
                "        all_results.append({'url': url, 'entities': [], 'triples': []})\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"PROCESSING COMPLETE\")\n",
                "print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Generate Mermaid Diagrams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate and save Mermaid diagrams\n",
                "for i, result in enumerate(all_results, 1):\n",
                "    filename = f\"mermaid_{i}.md\"\n",
                "    \n",
                "    if result['entities'] and result['triples']:\n",
                "        title = result['url'].split('/')[-1][:50] or f\"Document {i}\"\n",
                "        mermaid_content = triples_to_mermaid(result['triples'], result['entities'], title)\n",
                "    elif result['entities']:\n",
                "        mermaid_content = \"graph TD\\n    subgraph Entities\\n\"\n",
                "        for e in result['entities'][:10]:\n",
                "            mermaid_content += f\"        {clean_node_id(e.entity)}[\\\"{clean_label(e.entity)}\\\"]\\n\"\n",
                "        mermaid_content += \"    end\"\n",
                "    else:\n",
                "        mermaid_content = \"graph TD\\n    note[No entities extracted]\"\n",
                "    \n",
                "    with open(filename, 'w') as f:\n",
                "        f.write(f\"```mermaid\\n{mermaid_content}\\n```\\n\")\n",
                "    \n",
                "    print(f\"✓ Saved {filename}\")\n",
                "\n",
                "print(\"\\nAll Mermaid diagrams saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Generate CSV Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate tags.csv\n",
                "rows = []\n",
                "for result in all_results:\n",
                "    url = result['url']\n",
                "    seen_tags = set()\n",
                "    \n",
                "    for entity in result['entities']:\n",
                "        tag_key = entity.entity.lower().strip()\n",
                "        if tag_key not in seen_tags:\n",
                "            seen_tags.add(tag_key)\n",
                "            rows.append({\n",
                "                'link': url,\n",
                "                'tag': entity.entity,\n",
                "                'tag_type': entity.attr_type\n",
                "            })\n",
                "\n",
                "# Create DataFrame and save\n",
                "df = pd.DataFrame(rows)\n",
                "df.to_csv('tags.csv', index=False)\n",
                "\n",
                "print(f\"✓ Saved tags.csv with {len(rows)} entries\")\n",
                "print(\"\\nPreview:\")\n",
                "display(df.head(20))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Preview Mermaid Diagrams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display first mermaid diagram\n",
                "with open('mermaid_1.md', 'r') as f:\n",
                "    content = f.read()\n",
                "    display(Markdown(content))\n",
                "    print(\"\\nNote: Mermaid rendering requires a compatible viewer.\")\n",
                "    print(\"Copy the content to https://mermaid.live to visualize.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"PIPELINE SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Total URLs processed: {len(all_results)}\")\n",
                "print(f\"URLs with entities: {sum(1 for r in all_results if r['entities'])}\")\n",
                "print(f\"Total entities: {sum(len(r['entities']) for r in all_results)}\")\n",
                "print(f\"Total relationships: {sum(len(r['triples']) for r in all_results)}\")\n",
                "print(\"\\nOutputs:\")\n",
                "print(\"  - mermaid_1.md through mermaid_10.md\")\n",
                "print(\"  - tags.csv\")\n",
                "\n",
                "# Entity type distribution\n",
                "type_counts = {}\n",
                "for r in all_results:\n",
                "    for e in r['entities']:\n",
                "        type_counts[e.attr_type] = type_counts.get(e.attr_type, 0) + 1\n",
                "\n",
                "print(\"\\nEntity Types Distribution:\")\n",
                "for t, c in sorted(type_counts.items(), key=lambda x: -x[1])[:10]:\n",
                "    print(f\"  {t}: {c}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Pydantic + DSPy** = Structured outputs without regex parsing\n",
                "2. **Confidence loops** = Self-correcting LLM calls for critical tasks\n",
                "3. **Entity validation** = Prevents garbage nodes in knowledge graphs\n",
                "\n",
                "This pipeline demonstrates production-ready patterns for converting unstructured text into structured, queryable data."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
