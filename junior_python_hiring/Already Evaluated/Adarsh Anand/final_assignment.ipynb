{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Practical Assignment: Structuring Unstructured Data\n",
    "\n",
    "This notebook implements the full assignment: scraping 10 URLs, processing text with DSPy pipeline, and generating deliverables (Mermaid diagrams, CSV, and this notebook).\n",
    "\n",
    "## Setup\n",
    "- Install dependencies.\n",
    "- Set up API key (replace with your own from the assignment instructions).\n",
    "- Configure DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "#!pip install dspy-ai requests beautifulsoup4  # Install required packages: DSPy for AI pipelines, requests for HTTP, BeautifulSoup for HTML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # For handling JSON data structures\n",
    "import dspy  # Import DSPy library for AI pipeline components\n",
    "import copy  # For deep copying objects if needed\n",
    "from typing import List, Optional  # Type hints for lists and optional types\n",
    "from typing import Literal, Dict, Union  # Additional type hints for literals, dictionaries, and unions\n",
    "from dspy.adapters import XMLAdapter  # Import XML adapter for DSPy\n",
    "import requests  # For making HTTP requests to scrape websites\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import csv  # For writing CSV files with extracted data\n",
    "\n",
    "# API Key Setup: Replace with your own key from https://scribehow.com/viewer/Sign_Up_for_Longcat_API_Platform__9sYiobPNS0OnXzyxKHu4zg?add_to_team_with_invite=True&sharer_domain=gmail.com&sharer_id=e0b8270f-e494-45b1-b41a-c6adf9f11845\n",
    "API_KEY = 'ak_1Nm8PI6Zb0aR97X1wc9Zs32K8DZ06'  # Set the API key for LongCat API access\n",
    "\n",
    "main_lm = dspy.LM(\"openai/LongCat-Flash-Chat\", api_key=API_KEY, api_base=\"https://api.longcat.chat/openai/v1\")  # Initialize the language model with LongCat API\n",
    "\n",
    "dspy.settings.configure(lm=main_lm, adapter=dspy.XMLAdapter())  # Configure DSPy settings with the language model and XML adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPy Pipeline Components\n",
    "\n",
    "Define the signatures and predictors for entity extraction, deduplication, and relation extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field  # Import Pydantic for data validation and settings management\n",
    "\n",
    "class EntityWithAttr(BaseModel):# Data model for an entity with its semantic attribute type\n",
    "    entity: str = Field(description=\"the named entity\")# the named entity\n",
    "    attr_type: str = Field(description=\"semantic type of the entity (e.g. Drug, Disease, Symptom, etc.)\")# semantic type of the entity (e.g. Drug, Disease, Symptom, etc.)\n",
    "\n",
    "class ExtractEntities(dspy.Signature):\n",
    "    \"\"\"From the paragraph extract all relevant entities and their semantic attribute types.\"\"\"\n",
    "    paragraph: str = dspy.InputField(desc=\"input paragraph\")# input paragraph\n",
    "    entities: List[EntityWithAttr] = dspy.OutputField(desc=\"list of entities and their attribute types\")# list of entities and their attribute types\n",
    "\n",
    "# Predictor for extracting entities from text\n",
    "extractor = dspy.Predict(ExtractEntities)\n",
    "\n",
    "# Deduplication logic\n",
    "class DeduplicateEntities(dspy.Signature):\n",
    "    \"\"\"Given a list of (entity, attr_type) decide which ones are duplicates.\n",
    "    Return a deduplicated list and a confidence that the remaining items are ALL distinct.\"\"\"\n",
    "    items: List[EntityWithAttr] = dspy.InputField(desc=\"batch of entities to deduplicate\")# batch of entities to deduplicate\n",
    "    deduplicated: List[EntityWithAttr] = dspy.OutputField(desc=\"deduplicated list\")# deduplicated list\n",
    "    confidence: float = dspy.OutputField(\n",
    "        desc=\"confidence (0-1) that every item in deduplicated is semantically distinct\"# confidence (0-1) that every item in deduplicated is semantically distinct \n",
    "    )\n",
    "\n",
    "# Predictor for deduplicating entities using chain of thought reasoning\n",
    "dedup_predictor = dspy.ChainOfThought(DeduplicateEntities)# Predictor for deduplicating entities using chain of thought reasoning\n",
    "\n",
    "# Recursive deduplication function\n",
    "def deduplicate_with_lm(\n",
    "    items: List[EntityWithAttr],\n",
    "    *,\n",
    "    batch_size: int = 10,\n",
    "    target_confidence: float = 0.9,\n",
    ") -> List[EntityWithAttr]:\n",
    "    \"\"\"\n",
    "    Recursively deduplicate using the LM.\n",
    "    Works by:\n",
    "      1. splitting into batches of `batch_size`\n",
    "      2. for each batch asking the LM for duplicates + confidence\n",
    "      3. rerunning the batch until confidence >= target_confidence\n",
    "      4. concatenating results from all batches\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return []\n",
    "\n",
    "    # helper to process one batch\n",
    "    def _process_batch(batch: List[EntityWithAttr]) -> List[EntityWithAttr]:\n",
    "        while True:\n",
    "            pred = dedup_predictor(items=batch)\n",
    "            if pred.confidence >= target_confidence:\n",
    "                return pred.deduplicated\n",
    "            # otherwise loop again with same batch\n",
    "\n",
    "    # split into batches and process\n",
    "    results = []\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i : i + batch_size]\n",
    "        results.extend(_process_batch(batch))\n",
    "    return results\n",
    "\n",
    "# Data model for a subject-predicate-object relation\n",
    "class Relation(BaseModel):\n",
    "    subj: str = Field(description=\"subject entity (exact string as in deduplicated list)\")\n",
    "    pred: str = Field(description=\"short predicate / relation phrase\")\n",
    "    obj:  str = Field(description=\"object entity (exact string as in deduplicated list)\")\n",
    "\n",
    "# Signature for extracting relations from a paragraph given a list of entities\n",
    "class ExtractRelations(dspy.Signature):\n",
    "    \"\"\"Given the original paragraph and a list of unique entities, extract all factual (subject, predicate, object) triples that are explicitly stated or clearly implied.\"\"\"\n",
    "    paragraph: str = dspy.InputField(desc=\"original paragraph\")\n",
    "    entities:  List[str] = dspy.InputField(desc=\"list of deduplicated entity strings\")\n",
    "    relations: List[Relation] = dspy.OutputField(desc=\"list of subject-predicate-object triples\")\n",
    "\n",
    "# Predictor for extracting relations using chain of thought reasoning\n",
    "rel_predictor = dspy.ChainOfThought(ExtractRelations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mermaid Serialization\n",
    "\n",
    "Function to convert relations to Mermaid diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert triples to Mermaid flowchart\n",
    "def triples_to_mermaid(\n",
    "    triples: list[Relation],\n",
    "    entity_list: list[str],\n",
    "    max_label_len: int = 40\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convert triples to a VALID Mermaid flowchart LR diagram.\n",
    "    \"\"\"\n",
    "    entity_set = {e.strip().lower() for e in entity_list}\n",
    "    lines = [\"flowchart LR\"]\n",
    "\n",
    "    def _make_id(s: str) -> str:\n",
    "        # Create valid Mermaid node ID (no spaces or special chars)\n",
    "        return s.strip().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "    for t in triples:\n",
    "        subj_norm, obj_norm = t.subj.strip().lower(), t.obj.strip().lower()\n",
    "\n",
    "        if obj_norm in entity_set:\n",
    "            src, dst, lbl = t.subj, t.obj, t.pred\n",
    "        elif subj_norm in entity_set:\n",
    "            src, dst, lbl = t.obj, t.subj, t.pred\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Sanitize label\n",
    "        lbl = lbl.strip()\n",
    "        if len(lbl) > max_label_len:\n",
    "            lbl = lbl[:max_label_len - 3] + \"...\"\n",
    "\n",
    "        # Use valid IDs with display labels\n",
    "        src_id, dst_id = _make_id(src), _make_id(dst)\n",
    "        lines.append(f'    {src_id}[\"{src}\"] -->|{lbl}| {dst_id}[\"{dst}\"]')\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs and Processing\n",
    "\n",
    "List of URLs to scrape. Note: Some URLs may be invalid (future dates or typos); the code will skip them with error logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL 1: https://en.wikipedia.org/wiki/Sustainable_agriculture\n",
      "Completed URL 1\n",
      "Processing URL 2: https://www.nature.com/articles/d41586-025-03353-5\n",
      "Completed URL 2\n",
      "Processing URL 3: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
      "Error processing URL 3 (https://www.sciencedirect.com/science/article/pii/S1043661820315152): 400 Client Error: Bad Request for url: https://www.sciencedirect.com/unsupported_browser\n",
      "Processing URL 4: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\n",
      "Error processing URL 4 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/): 403 Client Error: Forbidden for url: https://pmc.ncbi.nlm.nih.gov/articles/PMC10457221/\n",
      "Processing URL 5: https://www.fao.org/3/y4671e/y4671e06.htm\n",
      "Completed URL 5\n",
      "Processing URL 6: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
      "Error processing URL 6 (https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria): 403 Client Error: Forbidden for url: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
      "Processing URL 7: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
      "Error processing URL 7 (https://www.sciencedirect.com/science/article/pii/S0378378220307088): 400 Client Error: Bad Request for url: https://www.sciencedirect.com/unsupported_browser\n",
      "Processing URL 8: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\n",
      "Completed URL 8\n",
      "Processing URL 9: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
      "Error processing URL 9 (https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7): 403 Client Error: Forbidden for url: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
      "Processing URL 10: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\n",
      "Completed URL 10\n",
      "Processing complete. Check files: mermaid_1.md to mermaid_10.md and tags.csv\n"
     ]
    }
   ],
   "source": [
    "# List of URLs to process\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
    "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
    "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
    "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
    "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
    "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
    "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
    "]\n",
    "\n",
    "csv_data = []# List to hold CSV rows\n",
    "\n",
    "for i, url in enumerate(urls, 1):\n",
    "    try:\n",
    "        print(f\"Processing URL {i}: {url}\")\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract text from paragraphs (basic scraping; may need refinement for complex sites)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "        if not text:\n",
    "            raise ValueError(\"No text extracted\")\n",
    "        # Truncate if too long (DSPy may have limits)\n",
    "        if len(text) > 10000:\n",
    "            text = text[:10000] + \"...\"\n",
    "        \n",
    "        # Extract entities\n",
    "        extracted = extractor(paragraph=text)\n",
    "        \n",
    "        # Deduplicate\n",
    "        unique = deduplicate_with_lm(extracted.entities)\n",
    "        \n",
    "        # Add to CSV data (deduplicated per URL)\n",
    "        for e in unique:\n",
    "            csv_data.append({'link': url, 'tag': e.entity, 'tag_type': e.attr_type})\n",
    "        \n",
    "        # Entity strings for relations\n",
    "        entity_strings = [e.entity for e in unique]\n",
    "        \n",
    "        # Extract relations\n",
    "        rel_out = rel_predictor(paragraph=text, entities=entity_strings)\n",
    "        \n",
    "        # Generate Mermaid\n",
    "        mermaid_code = triples_to_mermaid(rel_out.relations, entity_strings)\n",
    "        \n",
    "        # Save Mermaid diagram\n",
    "        with open(f'mermaid_{i}.md', 'w') as f:\n",
    "            f.write(\"```mermaid\\n\" + mermaid_code + \"\\n```\")\n",
    "        \n",
    "        print(f\"Completed URL {i}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {i} ({url}): {e}\")\n",
    "        # Skip invalid URLs\n",
    "\n",
    "# Save CSV\n",
    "with open('tags.csv', 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['link', 'tag', 'tag_type'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"Processing complete. Check files: mermaid_1.md to mermaid_10.md and tags.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
