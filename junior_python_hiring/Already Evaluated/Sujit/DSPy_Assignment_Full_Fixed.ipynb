{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93eff82",
   "metadata": {},
   "source": [
    "# DSPy Practical Assignment — Full Fixed Notebook\n",
    "\n",
    "This Colab-ready notebook implements a robust pipeline to:\n",
    "- Scrape 10 provided URLs\n",
    "- Extract entities using an LLM (strict JSON prompts, robust parsing)\n",
    "- Deduplicate entities (LLM confidence loop with deterministic fallback)\n",
    "- Extract relations (LLM placeholder)\n",
    "- Generate Mermaid diagrams (`mermaid_1.md` ... `mermaid_10.md`)\n",
    "- Produce `tags.csv`, `pipeline_log.json`, and `outputs.zip`\n",
    "\n",
    "**Before running**\n",
    "1. Replace `LONGCAT_API_ENDPOINT` and `LLM_API_KEY` in the **Configuration** cell.\n",
    "2. If your LLM provider returns a different JSON shape, update `call_llm()` accordingly.\n",
    "\n",
    "Run cells sequentially (top → bottom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests beautifulsoup4 newspaper3k pandas tqdm pydantic nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- CONFIG: Replace these with your provider details -----\n",
    "LONGCAT_API_ENDPOINT = \"https://api.longcat.example/v1/generate\"   # << REPLACE with real endpoint\n",
    "LLM_API_KEY = \"LLM_API_KEY_PLACEHOLDER\"                            # << REPLACE with your API key\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Standard imports\n",
    "import os, re, json, time, zipfile, requests\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Outputs\n",
    "TAGS_CSV = \"tags.csv\"\n",
    "PIPELINE_LOG = \"pipeline_log.json\"\n",
    "MERMAID_TEMPLATE = \"mermaid_{}.md\"\n",
    "\n",
    "# URLs to process\n",
    "URLS = [\n",
    "    'https://en.wikipedia.org/wiki/Sustainable_agriculture',\n",
    "    'https://www.nature.com/articles/d41586-025-03353-5',\n",
    "    'https://www.sciencedirect.com/science/article/pii/S1043661820315152',\n",
    "    'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/',\n",
    "    'https://www.fao.org/3/y4671e/y4671e06.htm',\n",
    "    'https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria',\n",
    "    'https://www.sciencedirect.com/science/article/pii/S0378378220307088',\n",
    "    'https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets',\n",
    "    'https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7',\n",
    "    'https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india'\n",
    "]\n",
    "\n",
    "ALLOWED_TYPES = ['Concept','Crop','Process','Measurement','Drug','Disease','Person','Location','Org','Date','StudyFinding','Other']\n",
    "\n",
    "# pipeline log\n",
    "pipeline_log = {\"urls\":{}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e943156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pydantic models\n",
    "class EntityWithAttr(BaseModel):\n",
    "    entity: str = Field(...)\n",
    "    attr_type: str = Field(...)\n",
    "\n",
    "class DedupResultMember(BaseModel):\n",
    "    canonical: str\n",
    "    members: List[str]\n",
    "    reason: str\n",
    "\n",
    "class DedupResponse(BaseModel):\n",
    "    deduplicated: List[DedupResultMember]\n",
    "    confidence: float\n",
    "\n",
    "class Triple(BaseModel):\n",
    "    src: str\n",
    "    label: str\n",
    "    dst: str\n",
    "\n",
    "# Fetch article text with fallback\n",
    "def fetch_article_text(url: str) -> Dict[str,Any]:\n",
    "    notes = []\n",
    "    text = \"\"\n",
    "    try:\n",
    "        art = Article(url)\n",
    "        art.download()\n",
    "        art.parse()\n",
    "        text = art.text or \"\"\n",
    "        if text and len(text) > 200:\n",
    "            notes.append(\"newspaper3k_full\")\n",
    "            return {\"text\": re.sub(r\"\\s+\",\" \", text).strip(), \"notes\": notes}\n",
    "    except Exception:\n",
    "        notes.append(\"newspaper3k_failed\")\n",
    "    try:\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "        r = requests.get(url, headers=headers, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        article = soup.find(\"article\")\n",
    "        if article:\n",
    "            paras = article.find_all(['p','h1','h2','h3'])\n",
    "            text = \" \".join(p.get_text(separator=\" \", strip=True) for p in paras)\n",
    "            notes.append(\"bs_article\")\n",
    "        else:\n",
    "            paras = soup.find_all('p')\n",
    "            text = \" \".join(p.get_text(separator=\" \", strip=True) for p in paras)\n",
    "            notes.append(\"bs_p\")\n",
    "        text = re.sub(r\"\\s+\",\" \", text).strip()\n",
    "        if len(text) < 100:\n",
    "            notes.append(\"short_text\")\n",
    "        return {\"text\": text, \"notes\": notes}\n",
    "    except Exception as e:\n",
    "        notes.append(f\"requests_failed:{str(e)[:120]}\")\n",
    "        return {\"text\":\"\", \"notes\": notes}\n",
    "\n",
    "# Chunking\n",
    "def chunk_text(text: str, max_chars: int = 3000) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    paras = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
    "    chunks = []\n",
    "    cur = \"\"\n",
    "    for p in paras:\n",
    "        if len(cur) + len(p) + 1 <= max_chars:\n",
    "            cur = (cur + \" \" + p).strip()\n",
    "        else:\n",
    "            if cur: chunks.append(cur)\n",
    "            cur = p\n",
    "    if cur: chunks.append(cur)\n",
    "    # fallback by sentences\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= max_chars:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            sents = re.split(r'(?<=[.!?])\\s+', c)\n",
    "            buff = \"\"\n",
    "            for s in sents:\n",
    "                if len(buff) + len(s) + 1 <= max_chars:\n",
    "                    buff = (buff + \" \" + s).strip()\n",
    "                else:\n",
    "                    final.append(buff)\n",
    "                    buff = s\n",
    "            if buff:\n",
    "                final.append(buff)\n",
    "    return final\n",
    "\n",
    "# Normalization & deterministic clustering\n",
    "def normalize_string(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.strip('\\\"\\' `.,;:-()[]{}')\n",
    "    return s.lower()\n",
    "\n",
    "def similar_ratio(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def deterministic_clusters(items: List[str]) -> List[Dict[str,Any]]:\n",
    "    norm_map = {i: normalize_string(i) for i in items}\n",
    "    clusters = []\n",
    "    used = set()\n",
    "    for i,item in enumerate(items):\n",
    "        if i in used: continue\n",
    "        key = norm_map[item]\n",
    "        group = [item]\n",
    "        used.add(i)\n",
    "        for j,item2 in enumerate(items[i+1:], start=i+1):\n",
    "            if j in used: continue\n",
    "            k2 = norm_map[item2]\n",
    "            set1=set(key.split()); set2=set(k2.split())\n",
    "            jacc = len(set1 & set2)/max(1,len(set1|set2))\n",
    "            ratio = similar_ratio(key,k2)\n",
    "            if ratio >= 0.85 or jacc >= 0.75:\n",
    "                group.append(item2); used.add(j)\n",
    "        clusters.append({\"canonical\": max(group, key=len), \"members\": group, \"reason\":\"heuristic\"})\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0887ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use existing dedup and resolve functions if present; else fallback to deterministic methods.\n",
    "try:\n",
    "    deduplicate_with_lm\n",
    "except NameError:\n",
    "    def deduplicate_with_lm(items, target_confidence=0.9, max_retries=3):\n",
    "        return deterministic_clusters(items)\n",
    "\n",
    "try:\n",
    "    resolve_type_with_llm\n",
    "except NameError:\n",
    "    def resolve_type_with_llm(entity, candidates):\n",
    "        if candidates:\n",
    "            counts={}\n",
    "            for c in candidates: counts[c]=counts.get(c,0)+1\n",
    "            top = sorted(counts.items(), key=lambda x:-x[1])[0][0]\n",
    "            return {\"entity\": entity, \"attr_type\": top, \"confidence\":0.9}\n",
    "        return {\"entity\": entity, \"attr_type\":\"Other\", \"confidence\":0.5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc68fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_llm(prompt: str, max_tokens: int = 800, temperature: float = 0.0, timeout: int = 30) -> str:\n",
    "    headers = {\"Authorization\": f\"Bearer {LLM_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"longcat-large\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(LONGCAT_API_ENDPOINT, headers=headers, json=payload, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        if isinstance(j, dict):\n",
    "            if \"choices\" in j and isinstance(j[\"choices\"], list) and \"text\" in j[\"choices\"][0]:\n",
    "                return j[\"choices\"][0][\"text\"]\n",
    "            if \"text\" in j:\n",
    "                return j[\"text\"]\n",
    "            if \"output\" in j:\n",
    "                return j[\"output\"]\n",
    "            if \"data\" in j:\n",
    "                return json.dumps(j[\"data\"])\n",
    "        return json.dumps(j)\n",
    "    except Exception as e:\n",
    "        return f\"#LLM_ERROR# {str(e)}\"\n",
    "\n",
    "ENTITY_PROMPT_TEMPLATE_SAFE = (\n",
    "    \"IMPORTANT — Output MUST be ONLY valid JSON (no explanation, no text). \"\n",
    "    \"Return a JSON array of objects. Each object must have exactly two keys: \"\n",
    "    \"\\\"entity\\\" and \\\"attr_type\\\". Allowed attr_type values: \" + str(ALLOWED_TYPES) + \".\\n\"\n",
    "    \"Example: [{\\\"entity\\\":\\\"sustainable agriculture\\\",\\\"attr_type\\\":\\\"Concept\\\"}]\\n\\n\"\n",
    "    \"Now extract entities from the following text and output JSON only:\\n\\n\"\n",
    "    \"{TEXT_REPLACE}\\n\"\n",
    ")\n",
    "\n",
    "def extract_entities_with_llm(text_chunk: str, max_retries: int = 1) -> List[EntityWithAttr]:\n",
    "    def _try_parse(raw: str):\n",
    "        try:\n",
    "            return json.loads(raw)\n",
    "        except Exception:\n",
    "            pass\n",
    "        m = re.search(r'(\\[\\s*\\{.*?\\}\\s*\\])', raw, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            j = json.loads(raw)\n",
    "            if isinstance(j, dict):\n",
    "                for k in (\"entities\",\"data\",\"result\",\"output\",\"items\"):\n",
    "                    if k in j and isinstance(j[k], list):\n",
    "                        return j[k]\n",
    "        except Exception:\n",
    "            pass\n",
    "        objs=[]\n",
    "        for ln in raw.splitlines():\n",
    "            ln = ln.strip()\n",
    "            if ln.startswith(\"{\") and ln.endswith(\"}\"):\n",
    "                try:\n",
    "                    objs.append(json.loads(ln))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if objs: return objs\n",
    "        return None\n",
    "\n",
    "    prompt = ENTITY_PROMPT_TEMPLATE_SAFE.replace(\"{TEXT_REPLACE}\", text_chunk[:4000])\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        raw = call_llm(prompt, max_tokens=800, temperature=0.0)\n",
    "        raw_str = (raw or \"\").strip()\n",
    "        pipeline_log.setdefault(\"llm_raw_samples\", []).append({\n",
    "            \"attempt\": attempt, \"preview_text\": text_chunk[:200], \"raw_preview\": raw_str[:2000], \"ts\": time.time()\n",
    "        })\n",
    "        print(f\"[LLM raw preview attempt {attempt}]\")\n",
    "        print(raw_str[:1000])\n",
    "        print(\"-\"*70)\n",
    "        parsed = _try_parse(raw_str)\n",
    "        if parsed is None:\n",
    "            if attempt <= max_retries:\n",
    "                prompt = ENTITY_PROMPT_TEMPLATE_SAFE.replace(\"{TEXT_REPLACE}\", text_chunk[:4000])\n",
    "                continue\n",
    "            pipeline_log.setdefault(\"extract_errors\", []).append({\"reason\":\"no_json_parsed\",\"raw_preview\":raw_str[:2000],\"ctx\":text_chunk[:200]})\n",
    "            return []\n",
    "        if isinstance(parsed, dict):\n",
    "            for k in (\"entities\",\"data\",\"result\",\"output\",\"items\"):\n",
    "                if k in parsed and isinstance(parsed[k], list):\n",
    "                    parsed = parsed[k]; break\n",
    "            else:\n",
    "                parsed = [parsed]\n",
    "        if not isinstance(parsed, list):\n",
    "            if attempt <= max_retries:\n",
    "                prompt = ENTITY_PROMPT_TEMPLATE_SAFE.replace(\"{TEXT_REPLACE}\", text_chunk[:4000])\n",
    "                continue\n",
    "            pipeline_log.setdefault(\"extract_errors\", []).append({\"reason\":\"parsed_not_list\",\"preview\":str(parsed)[:400]})\n",
    "            return []\n",
    "        results=[]; invalids=[]\n",
    "        for item in parsed:\n",
    "            if not isinstance(item, dict):\n",
    "                invalids.append({\"preview\":str(item)[:200],\"reason\":\"not_dict\"}); continue\n",
    "            if \"entity\" not in item or \"attr_type\" not in item:\n",
    "                invalids.append({\"keys\": list(item.keys()), \"preview\":str(item)[:200], \"reason\":\"missing_keys\"}); continue\n",
    "            try:\n",
    "                ent = EntityWithAttr(entity=str(item[\"entity\"]).strip(), attr_type=str(item[\"attr_type\"]).strip())\n",
    "                if ent.attr_type not in ALLOWED_TYPES: ent.attr_type = ent.attr_type.title()\n",
    "                results.append(ent)\n",
    "            except Exception as e:\n",
    "                invalids.append({\"preview\":str(item)[:200],\"reason\":\"validation_error\",\"error\":str(e)})\n",
    "                continue\n",
    "        if invalids:\n",
    "            pipeline_log.setdefault(\"entity_parse_invalid_items\", []).append({\"ctx\":text_chunk[:200],\"invalid_count\":len(invalids),\"examples\":invalids[:6],\"raw_preview\":raw_str[:1000]})\n",
    "            print(f\"Found {len(invalids)} invalid items; keeping {len(results)} valid ones.\")\n",
    "            if results: return results\n",
    "            if attempt <= max_retries:\n",
    "                prompt = ENTITY_PROMPT_TEMPLATE_SAFE.replace(\"{TEXT_REPLACE}\", text_chunk[:4000]); continue\n",
    "            return []\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def triples_to_mermaid(triples: List[Triple], entity_list: List[str]) -> str:\n",
    "    entity_set = {e.strip().lower() for e in entity_list}\n",
    "    nodes = {e: f\"n{i}\" for i,e in enumerate(entity_list, start=1)}\n",
    "    lines = [\"graph TD\"]\n",
    "    for e,idv in nodes.items():\n",
    "        label = e.replace('\"','\\\"')\n",
    "        lines.append(f'{idv}[\"{label}\"]')\n",
    "    for t in triples:\n",
    "        src=t.src; dst=t.dst; lbl=t.label[:40]\n",
    "        if src.strip().lower() in entity_set and dst.strip().lower() in entity_set:\n",
    "            lines.append(f'{nodes[src]} -- \"{lbl}\" --> {nodes[dst]}')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Placeholder relation extractor if not provided earlier\n",
    "try:\n",
    "    extract_triples_with_llm\n",
    "except NameError:\n",
    "    def extract_triples_with_llm(text, entities):\n",
    "        # conservative placeholder: return empty list; replace with your relation LLM call if available\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Orchestration: process each URL and save outputs\n",
    "all_tags_rows=[]\n",
    "for idx, url in enumerate(URLS, start=1):\n",
    "    print(\"\\nProcessing\", idx, url)\n",
    "    data = fetch_article_text(url)\n",
    "    text = data.get(\"text\",\"\")\n",
    "    notes = data.get(\"notes\",[])\n",
    "    pipeline_log[\"urls\"][url] = {\"notes\": notes}\n",
    "    if not text:\n",
    "        print(\"No text extracted for\", url); pipeline_log[\"urls\"][url][\"error\"]=\"no_text\"; continue\n",
    "    chunks = chunk_text(text, max_chars=3000); print(\"Chunks:\", len(chunks))\n",
    "    raw_entities=[]\n",
    "    for c_i, c in enumerate(chunks):\n",
    "        try:\n",
    "            ents = extract_entities_with_llm(c)\n",
    "        except Exception as e:\n",
    "            print(\"extract_entities_with_llm crashed on chunk\", c_i, \"recovering:\", e)\n",
    "            pipeline_log.setdefault(\"fatal_chunk_errors\", []).append({\"url\":url,\"chunk_index\":c_i,\"chunk_preview\":c[:200],\"error\":str(e)})\n",
    "            ents=[]\n",
    "        raw_entities.extend(ents)\n",
    "    occ_map={}\n",
    "    for e in raw_entities:\n",
    "        try:\n",
    "            key=e.entity.strip(); occ_map.setdefault(key,[]).append(e.attr_type)\n",
    "        except Exception:\n",
    "            pass\n",
    "    original_items=list(occ_map.keys()); print(\"Raw unique entity strings:\", len(original_items))\n",
    "    try:\n",
    "        dedup_output = deduplicate_with_lm(original_items, target_confidence=0.9, max_retries=3)\n",
    "    except Exception as e:\n",
    "        print(\"Dedup failed, falling back to deterministic clusters:\", e)\n",
    "        dedup_output = deterministic_clusters(original_items)\n",
    "    canonical_list=[d[\"canonical\"] for d in dedup_output]\n",
    "    canonical_types={}\n",
    "    for d in dedup_output:\n",
    "        candidates=[]\n",
    "        for m in d.get(\"members\",[]): candidates.extend(occ_map.get(m,[]))\n",
    "        try:\n",
    "            resolved = resolve_type_with_llm(d[\"canonical\"], candidates)\n",
    "            canonical_types[d[\"canonical\"]] = resolved.get(\"attr_type\", \"Other\")\n",
    "        except Exception:\n",
    "            canonical_types[d[\"canonical\"]] = \"Other\"\n",
    "    try:\n",
    "        triples = extract_triples_with_llm(text, canonical_list)\n",
    "    except Exception as e:\n",
    "        print(\"Relation extraction failed:\", e); triples=[]\n",
    "    print(\"Triples extracted:\", len(triples))\n",
    "    # verify by sentence co-occurrence\n",
    "    verified_triples=[]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    for t in triples:\n",
    "        ok=False\n",
    "        for s in sentences:\n",
    "            if (t.src in s) and (t.dst in s):\n",
    "                ok=True; break\n",
    "        if ok: verified_triples.append(t)\n",
    "    print(\"Verified triples after co-occurrence:\", len(verified_triples))\n",
    "    mermaid_text = triples_to_mermaid(verified_triples, canonical_list)\n",
    "    fname = MERMAID_TEMPLATE.format(idx)\n",
    "    try:\n",
    "        with open(fname, \"w\", encoding=\"utf-8\") as f: f.write(mermaid_text)\n",
    "        print(\"Saved\", fname)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to write mermaid file:\", e)\n",
    "    for can in canonical_list:\n",
    "        all_tags_rows.append({\"link\":url, \"tag\":can, \"tag_type\": canonical_types.get(can,\"Other\")})\n",
    "    pipeline_log[\"urls\"][url][\"num_raw_entities\"] = len(original_items)\n",
    "    pipeline_log[\"urls\"][url][\"num_canonical\"] = len(canonical_list)\n",
    "    pipeline_log[\"urls\"][url][\"num_triples\"] = len(verified_triples)\n",
    "\n",
    "# Save tags.csv and pipeline_log\n",
    "df = pd.DataFrame(all_tags_rows).drop_duplicates(subset=[\"link\",\"tag\"])\n",
    "df.to_csv(TAGS_CSV, index=False)\n",
    "with open(PIPELINE_LOG, \"w\", encoding=\"utf-8\") as f: json.dump(pipeline_log, f, ensure_ascii=False, indent=2)\n",
    "print(\"Wrote\", TAGS_CSV, \"and\", PIPELINE_LOG)\n",
    "\n",
    "# Zip outputs\n",
    "with zipfile.ZipFile(\"outputs.zip\",\"w\") as z:\n",
    "    if os.path.exists(TAGS_CSV): z.write(TAGS_CSV)\n",
    "    if os.path.exists(PIPELINE_LOG): z.write(PIPELINE_LOG)\n",
    "    for i in range(1, len(URLS)+1):\n",
    "        fn = MERMAID_TEMPLATE.format(i)\n",
    "        if os.path.exists(fn): z.write(fn)\n",
    "print(\"Created outputs.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick summary: counts per URL\n",
    "import pprint\n",
    "with open(PIPELINE_LOG,'r',encoding='utf-8') as f: log = json.load(f)\n",
    "summary = []\n",
    "for url,info in log.get(\"urls\",{}).items():\n",
    "    summary.append({\n",
    "        \"url\": url,\n",
    "        \"num_raw_entities\": info.get(\"num_raw_entities\"),\n",
    "        \"num_canonical\": info.get(\"num_canonical\"),\n",
    "        \"num_triples\": info.get(\"num_triples\"),\n",
    "        \"notes\": info.get(\"notes\",[])\n",
    "    })\n",
    "pprint.pprint(summary)\n",
    "print(\"\\nTags CSV preview:\")\n",
    "if os.path.exists(TAGS_CSV):\n",
    "    df = pd.read_csv(TAGS_CSV)\n",
    "    print(df.head(20))\n",
    "else:\n",
    "    print(\"No tags.csv found.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
