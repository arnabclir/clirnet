{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzZs0Ic0YLCp",
        "outputId": "a1c4aaa3-5505-4324-a35b-393d6724dfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.0/569.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# 1) Install dependencies\n",
        "!pip install -q requests beautifulsoup4 pandas spacy rdflib networkx matplotlib\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "# 2) Import all libraries\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import requests\n",
        "import textwrap\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import OrderedDict, defaultdict\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "\n",
        "try:\n",
        "    import dspy\n",
        "    DSPY_AVAILABLE = True\n",
        "except Exception:\n",
        "    DSPY_AVAILABLE = False\n",
        "\n",
        "\n",
        "OUTPUT_DIR = \"/content/dspy_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
        "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
        "]\n",
        "\n",
        "\n",
        "MAX_TAGS_PER_URL = 30\n",
        "\n",
        "\n",
        "EDGE_LABEL_MAX_LEN = 40"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Utilities: fetch page & extract main text\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible)\"}\n",
        "\n",
        "def fetch_page_text(url: str, timeout: int = 20) -> str:\n",
        "    \"\"\"\n",
        "    Fetch the page and extract visible paragraph text.\n",
        "    If fetch fails, returns empty string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        # Heuristics: gather paragraphs; prefer article/main if possible\n",
        "        main_text = []\n",
        "        # Try common selectors\n",
        "        selectors = [\"article\", \"main\", \"div[id='content']\", \"div.article-body\", \"div[itemprop='articleBody']\"]\n",
        "        for sel in selectors:\n",
        "            el = soup.select_one(sel)\n",
        "            if el:\n",
        "                main_text = [p.get_text(separator=\" \", strip=True) for p in el.find_all(\"p\")]\n",
        "                break\n",
        "        if not main_text:\n",
        "            main_text = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "        joined = \"\\n\\n\".join([t for t in main_text if t and len(t) > 20])\n",
        "        return re.sub(r\"\\s+\", \" \", joined).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[fetch_page_text] failed for {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# 4) DSPy pipeline wrapper\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def dspy_extract_entities(text: str, top_k: int = 50) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Attempt to run DSPy pipeline if available. Otherwise, run spaCy-based heuristics.\n",
        "    Returns list of (entity_string, candidate_type)\n",
        "    \"\"\"\n",
        "    if not text or len(text) < 50:\n",
        "        return []\n",
        "\n",
        "    if DSPY_AVAILABLE:\n",
        "        try:\n",
        "\n",
        "            dsp_results = dspy.run_pipeline(text, top_k=top_k)  # <-- replace with real call\n",
        "            ents = []\n",
        "            for item in dsp_results:\n",
        "                ent_text = item.get(\"text\") or item.get(\"entity\") or item.get(\"mention\")\n",
        "                ent_type = item.get(\"type\") or item.get(\"category\") or \"Concept\"\n",
        "                if ent_text:\n",
        "                    ents.append((ent_text.strip(), ent_type))\n",
        "            if ents:\n",
        "                return ents[:top_k]\n",
        "        except Exception as e:\n",
        "            print(\"[dspy_extract_entities] DSPy run failed, falling back to spaCy:\", e)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    ents = []\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        ent_text = ent.text.strip()\n",
        "        ent_label = ent.label_\n",
        "\n",
        "        if ent_label in (\"PERSON\", \"NORP\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\"):\n",
        "            ttype = \"Concept\"\n",
        "        elif ent_label in (\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"):\n",
        "            ttype = \"Measurement\"\n",
        "        elif ent_label in (\"LAW\",):\n",
        "            ttype = \"Policy\"\n",
        "        else:\n",
        "            ttype = \"Concept\"\n",
        "        ents.append((ent_text, ttype))\n",
        "\n",
        "    noun_chunks = [chunk.text.strip() for chunk in doc.noun_chunks]\n",
        "\n",
        "    freq = defaultdict(int)\n",
        "    for nc in noun_chunks:\n",
        "        freq[nc.lower()] += 1\n",
        "\n",
        "    sorted_ncs = sorted(freq.items(), key=lambda x: -x[1])\n",
        "    for nc, _ in sorted_ncs[:top_k]:\n",
        "        # find original chunk text that matches (first occurrence)\n",
        "        for chunk in noun_chunks:\n",
        "            if chunk.lower() == nc:\n",
        "                ents.append((chunk, \"Concept\"))\n",
        "                break\n",
        "\n",
        "    seen = set(); out = []\n",
        "    for v,t in ents:\n",
        "        key = v.lower().strip()\n",
        "        if key in seen: continue\n",
        "        seen.add(key)\n",
        "        out.append((v,t))\n",
        "        if len(out) >= top_k: break\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "8jkOHHowLNWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Canonicalization and deduplication rules\n",
        "\n",
        "CANONICAL_MAP = {\n",
        "    \"ageing\": \"aging\",\n",
        "    \"ageing.\": \"aging\",\n",
        "    \"ageing,\": \"aging\",\n",
        "    \"micro nutrients\": \"micronutrients\",\n",
        "    \"micro-nutrients\": \"micronutrients\",\n",
        "    \"ivermectin\": \"ivermectin\"\n",
        "}\n",
        "\n",
        "def canonicalize_tag(tag: str) -> str:\n",
        "    \"\"\"\n",
        "    Return canonical lowercase key for deduplication and mapping.\n",
        "    Applies mapping rules like 'ageing' -> 'aging', removes extra whitespace and punctuation.\n",
        "    \"\"\"\n",
        "    t = tag.strip()\n",
        "    t = re.sub(r\"^[\\\"'`]+|[\\\"'`.?,;!]+$\", \"\", t)\n",
        "    low = t.lower()\n",
        "    if low in CANONICAL_MAP:\n",
        "        return CANONICAL_MAP[low]\n",
        "\n",
        "    low = low.replace(\"—\", \"-\").replace(\"/\", \" \").strip()\n",
        "\n",
        "    if low.endswith(\"ies\"):\n",
        "        low = low[:-3] + \"y\"\n",
        "    elif low.endswith(\"s\") and not low.endswith(\"ss\"):\n",
        "        low = low[:-1]\n",
        "    return low\n",
        "\n",
        "\n",
        "# 6) Post-processing\n",
        "\n",
        "def map_tag_type(tag: str, candidate: str) -> str:\n",
        "    \"\"\"\n",
        "    Return a tag_type string for `tag`.\n",
        "    'candidate' is the initial candidate type from DSPy/spaCy (e.g., 'Concept', 'Measurement')\n",
        "    This mapping is intentionally small; extend for your taxonomy.\n",
        "    \"\"\"\n",
        "    t = candidate or \"\"\n",
        "    tag_lower = tag.lower()\n",
        "    if any(x in tag_lower for x in [\"vaccine\", \"drug\", \"ivermectin\", \"tramadol\", \"antibiotic\", \"compound\"]):\n",
        "        return \"Drug\"\n",
        "    if any(x in tag_lower for x in [\"plant\", \"crop\", \"pea\", \"barley\", \"agroforestry\", \"permaculture\"]):\n",
        "        return \"Crop\"\n",
        "    if any(x in tag_lower for x in [\"process\", \"rotation\", \"uptake\", \"fixation\", \"preparation\", \"dosing\", \"booster\"]):\n",
        "        return \"Process\"\n",
        "    if any(x in tag_lower for x in [\"study\", \"trial\", \"longitudinal\", \"method\", \"task\"]):\n",
        "        return \"Method\"\n",
        "    if any(x in tag_lower for x in [\"planet\", \"exoplanet\", \"habitable\", \"telescope\", \"instrument\"]):\n",
        "        return \"Instrument\"\n",
        "    if any(x in tag_lower for x in [\"mortality\", \"incidence\", \"prevalence\", \"rate\", \"measure\", \"%\", \"confidence\"]):\n",
        "        return \"Measurement\"\n",
        "    if any(x in tag_lower for x in [\"population\", \"adults\", \"children\", \"aged\"]):\n",
        "        return \"Population\"\n",
        "    if \"location\" in tag_lower or \",\" in tag:\n",
        "        return \"Location\"\n",
        "\n",
        "    return t if t else \"Concept\""
      ],
      "metadata": {
        "id": "SkSkJLC0LOHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Per-URL processing\n",
        "\n",
        "rows = []\n",
        "\n",
        "\n",
        "per_url_deduped_tags = {}\n",
        "\n",
        "for url in URLS:\n",
        "    print(f\"Processing URL: {url}\")\n",
        "    text = fetch_page_text(url)\n",
        "    entities = dspy_extract_entities(text, top_k=MAX_TAGS_PER_URL)\n",
        "\n",
        "    deduped = OrderedDict()\n",
        "    for ent_text, cand_type in entities:\n",
        "        if not ent_text or len(ent_text) < 2:\n",
        "            continue\n",
        "\n",
        "        tag_exact = ent_text.strip()\n",
        "        tag_canonical = canonicalize_tag(tag_exact)\n",
        "\n",
        "        if tag_canonical in deduped:\n",
        "\n",
        "\n",
        "          continue\n",
        "\n",
        "        assigned_type = map_tag_type(tag_exact, cand_type)\n",
        "        deduped[tag_canonical] = {\"tag\": tag_exact, \"tag_type\": assigned_type}\n",
        "\n",
        "    per_url_deduped_tags[url] = list(deduped.values())\n",
        "    for info in per_url_deduped_tags[url]:\n",
        "        rows.append({\"link\": url, \"tag\": info[\"tag\"], \"tag_type\": info[\"tag_type\"]})\n",
        "\n",
        "# 8) Save tags.csv\n",
        "\n",
        "csv_path = os.path.join(OUTPUT_DIR, \"tags.csv\")\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=[\"link\", \"tag\", \"tag_type\"])\n",
        "    writer.writeheader()\n",
        "    for r in rows:\n",
        "        writer.writerow(r)\n",
        "\n",
        "print(\"Saved deduplicated tags.csv ->\", csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsRKz_7PMtpE",
        "outputId": "1e28aa52-a961-409f-f7b3-81eea2ce7fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing URL: https://en.wikipedia.org/wiki/Sustainable_agriculture\n",
            "Processing URL: https://www.nature.com/articles/d41586-025-03353-5\n",
            "Processing URL: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "[fetch_page_text] failed for https://www.sciencedirect.com/science/article/pii/S1043661820315152: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\n",
            "[fetch_page_text] failed for https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/: 403 Client Error: Forbidden for url: https://pmc.ncbi.nlm.nih.gov/articles/PMC10457221/\n",
            "Processing URL: https://www.fao.org/3/y4671e/y4671e06.htm\n",
            "[fetch_page_text] failed for https://www.fao.org/3/y4671e/y4671e06.htm: 504 Server Error: Gateway Timeout for url: https://www.fao.org/3/y4671e/y4671e06.htm\n",
            "Processing URL: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
            "Processing URL: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "[fetch_page_text] failed for https://www.sciencedirect.com/science/article/pii/S0378378220307088: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "Processing URL: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\n",
            "Processing URL: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
            "Processing URL: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\n",
            "Saved deduplicated tags.csv -> /content/dspy_output/tags.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Produce 10 Mermaid .md files\n",
        "\n",
        "def trim_label(label: str, maxlen: int = EDGE_LABEL_MAX_LEN) -> str:\n",
        "    if not label:\n",
        "        return \"\"\n",
        "    label = label.strip()\n",
        "    if len(label) <= maxlen:\n",
        "        return label\n",
        "    return label[:maxlen].rstrip() + \"…\"  # ellipsis indicates trimming\n",
        "\n",
        "for i, url in enumerate(URLS, start=1):\n",
        "    tags_for_url = per_url_deduped_tags.get(url, [])\n",
        "    filename = os.path.join(OUTPUT_DIR, f\"mermaid_{i}.md\")\n",
        "\n",
        "    lines = [\"```mermaid\", \"graph LR\"]\n",
        "\n",
        "    nodes = [info[\"tag\"] for info in tags_for_url]\n",
        "    if not nodes:\n",
        "\n",
        "        site_node = url.split(\"//\")[-1].split(\"/\")[0]\n",
        "        nodes = [site_node]\n",
        "\n",
        "    central = nodes[0]\n",
        "\n",
        "    for target in nodes[1:]:\n",
        "\n",
        "        t_central_type = None\n",
        "        t_target_type = None\n",
        "\n",
        "        for info in tags_for_url:\n",
        "            if info[\"tag\"] == central:\n",
        "                t_central_type = info.get(\"tag_type\")\n",
        "            if info[\"tag\"] == target:\n",
        "                t_target_type = info.get(\"tag_type\")\n",
        "\n",
        "        if t_central_type and t_target_type:\n",
        "            label = f\"{t_central_type} → {t_target_type}\"\n",
        "        else:\n",
        "            label = \"related to\"\n",
        "        label = trim_label(label, EDGE_LABEL_MAX_LEN)\n",
        "\n",
        "        lines.append(f'  \"{central}\" -->|\"{label}\"| \"{target}\"')\n",
        "    lines.append(\"```\")\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "    print(\"Wrote\", filename)\n",
        "\n",
        "\n",
        "# 10) Consolidate 10 Mermaids into one mermaid_all.md\n",
        "\n",
        "all_nodes = set()\n",
        "all_edges = set()\n",
        "\n",
        "for i in range(1, len(URLS)+1):\n",
        "    path = os.path.join(OUTPUT_DIR, f\"mermaid_{i}.md\")\n",
        "    if not os.path.exists(path):\n",
        "        continue\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    edge_lines = [ln.strip() for ln in content.splitlines() if \"-->\" in ln]\n",
        "    for ln in edge_lines:\n",
        "\n",
        "\n",
        "        m = re.match(r'\\s*\"([^\"]+)\"\\s*-->\\|\\s*\"([^\"]*)\"\\s*\\|\\s*\"([^\"]+)\"', ln)\n",
        "        if m:\n",
        "            a, label, b = m.group(1), m.group(2), m.group(3)\n",
        "            label = trim_label(label, EDGE_LABEL_MAX_LEN)\n",
        "            key = (a, label, b)\n",
        "            all_edges.add(key)\n",
        "            all_nodes.add(a); all_nodes.add(b)\n",
        "        else:\n",
        "\n",
        "            m2 = re.match(r'\\s*\"([^\"]+)\"\\s*-->\\s*\"([^\"]+)\"', ln)\n",
        "            if m2:\n",
        "                a, b = m2.group(1), m2.group(2)\n",
        "                key = (a, \"\", b)\n",
        "                all_edges.add(key)\n",
        "                all_nodes.add(a); all_nodes.add(b)\n",
        "\n",
        "\n",
        "consolidated_lines = [\"```mermaid\", \"graph LR\"]\n",
        "for a, label, b in sorted(all_edges):\n",
        "    if label:\n",
        "        consolidated_lines.append(f'  \"{a}\" -->|\"{label}\"| \"{b}\"')\n",
        "    else:\n",
        "        consolidated_lines.append(f'  \"{a}\" --> \"{b}\"')\n",
        "consolidated_lines.append(\"```\")\n",
        "\n",
        "consolidated_path = os.path.join(OUTPUT_DIR, \"mermaid_all.md\")\n",
        "with open(consolidated_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(consolidated_lines))\n",
        "\n",
        "print(\"Wrote consolidated mermaid ->\", consolidated_path)\n",
        "\n",
        "\n",
        "notebook_note = os.path.join(OUTPUT_DIR, \"README.txt\")\n",
        "with open(notebook_note, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(f\"\"\"\n",
        "    This directory contains outputs generated by the DSPy-style pipeline script.\n",
        "    Files:\n",
        "      - tags.csv                 (deduplicated tags per URL; columns: link, tag, tag_type)\n",
        "      - mermaid_1.md ... mermaid_10.md   (per-URL mermaid diagrams)\n",
        "      - mermaid_all.md           (consolidated mermaid diagram)\n",
        "    To reproduce, open the original script in Colab, run cells top-to-bottom.\n",
        "    \"\"\"))\n",
        "\n",
        "# 11) Summary\n",
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(\"Outputs saved in:\", OUTPUT_DIR)\n",
        "print(\" - tags.csv\")\n",
        "for i in range(1, len(URLS)+1):\n",
        "    print(f\" - mermaid_{i}.md\")\n",
        "print(\" - mermaid_all.md\")\n",
        "print(\"\\nOpen the Mermaid files in Mermaid Live Editor or paste the mermaid_block into a markdown file/viewer that supports Mermaid.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQTh0g_SNTDW",
        "outputId": "6db617e7-b8bd-43c5-e46e-b0f03472dece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/dspy_output/mermaid_1.md\n",
            "Wrote /content/dspy_output/mermaid_2.md\n",
            "Wrote /content/dspy_output/mermaid_3.md\n",
            "Wrote /content/dspy_output/mermaid_4.md\n",
            "Wrote /content/dspy_output/mermaid_5.md\n",
            "Wrote /content/dspy_output/mermaid_6.md\n",
            "Wrote /content/dspy_output/mermaid_7.md\n",
            "Wrote /content/dspy_output/mermaid_8.md\n",
            "Wrote /content/dspy_output/mermaid_9.md\n",
            "Wrote /content/dspy_output/mermaid_10.md\n",
            "Wrote consolidated mermaid -> /content/dspy_output/mermaid_all.md\n",
            "\n",
            "=== SUMMARY ===\n",
            "Outputs saved in: /content/dspy_output\n",
            " - tags.csv\n",
            " - mermaid_1.md\n",
            " - mermaid_2.md\n",
            " - mermaid_3.md\n",
            " - mermaid_4.md\n",
            " - mermaid_5.md\n",
            " - mermaid_6.md\n",
            " - mermaid_7.md\n",
            " - mermaid_8.md\n",
            " - mermaid_9.md\n",
            " - mermaid_10.md\n",
            " - mermaid_all.md\n",
            "\n",
            "Open the Mermaid files in Mermaid Live Editor or paste the mermaid_block into a markdown file/viewer that supports Mermaid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r archive.zip dspy_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOEnqV4zPB6D",
        "outputId": "ada13a68-4264-4003-93f4-947ae5f7c626"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: dspy_output/ (stored 0%)\n",
            "  adding: dspy_output/mermaid_4.md (stored 0%)\n",
            "  adding: dspy_output/mermaid_6.md (deflated 75%)\n",
            "  adding: dspy_output/mermaid_5.md (stored 0%)\n",
            "  adding: dspy_output/mermaid_2.md (deflated 81%)\n",
            "  adding: dspy_output/README.txt (deflated 38%)\n",
            "  adding: dspy_output/mermaid_all.md (deflated 85%)\n",
            "  adding: dspy_output/mermaid_1.md (deflated 80%)\n",
            "  adding: dspy_output/mermaid_10.md (deflated 78%)\n",
            "  adding: dspy_output/mermaid_7.md (stored 0%)\n",
            "  adding: dspy_output/mermaid_9.md (deflated 83%)\n",
            "  adding: dspy_output/mermaid_3.md (stored 0%)\n",
            "  adding: dspy_output/mermaid_8.md (deflated 82%)\n",
            "  adding: dspy_output/tags.csv (deflated 90%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8fWb_DtPn0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}